{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3bbe3b",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66faeb56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import imlreliability\n",
    "import pandas as pd\n",
    "import numpy as np#### Load Packages dir(imlreliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "shared-stationery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_version',\n",
       " 'clustering',\n",
       " 'dimension_reduction',\n",
       " 'feature_importance']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(imlreliability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9c219",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-prague",
   "metadata": {},
   "source": [
    "Reliability test of feature importance techniques can be performed with the module imlreliability.feature_importance. Non-MLP techniques can be evaluated ``feature_impoReg`` for regression tasks and ``feature_impoClass`` for classification tasks. MLP-based techniques can be evaluated by ``feature_impoReg_MLP`` for regression tasks and ``feature_impoClass_MLP`` for classification tasks. \n",
    "\n",
    "Model agnostic techniques can be evaluated by specifying the parameter of importance function ``importance_func``. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "under-dakota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_feature_impo',\n",
       " 'feature_impoClass',\n",
       " 'feature_impoClass_MLP',\n",
       " 'feature_impoReg',\n",
       " 'feature_impoReg_MLP',\n",
       " 'rbo',\n",
       " 'util_feature_impo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(imlreliability.feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-assist",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e7ef7",
   "metadata": {},
   "source": [
    "#### Load data\n",
    " We use the communities regression data as an example for the following sections. The data has 1993 observations and 99 feature. We pre-process the data set by scaling and normalizing the predictors and scaling the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beeaba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, normalize\n",
    "communities_data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data').to_numpy()\n",
    "communities_data = np.delete(communities_data, np.arange(5), 1)\n",
    "    # remove predictors with missing values\n",
    "communities_data = np.delete(communities_data,\n",
    "                             np.argwhere((communities_data == '?').sum(0) > 0).reshape(-1), 1)\n",
    "communities_data = communities_data.astype(float)\n",
    "x = communities_data[:, :-1]\n",
    "y = communities_data[:, -1]\n",
    "\n",
    "\n",
    "### scale and normalize data \n",
    "x = normalize(scale(x))\n",
    "y = (scale(y))\n",
    "data_reg=(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ce6ae",
   "metadata": {},
   "source": [
    "### 1.1. Model specific IML method\n",
    "\n",
    "The estimator is assumed to implement the scikit-learn estimator interface. To measure the feature importance, either estimator needs to provide a ``score`` function or ``scoring`` must be passed. For example, in linear regression, the magnitude of coefficients is used to evaluate feature importance if there is no user-defined scoring function provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc58e",
   "metadata": {},
   "source": [
    "#### 1.1.1. Linear model\n",
    "\n",
    "Here we aim to evaluate the interpretation reliability of Ridge regression with cross validation, using the ``feature_impoReg``function. We use ``RidgeCV()`` from ``sklearn`` as our estimator. By setting ``importance_func=None``, the magnitude of coefficients will be used to evaluate feature importance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7527818e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "estimator=RidgeCV()\n",
    "importance_func=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-divorce",
   "metadata": {},
   "source": [
    "We initialize the model with the ``mlreliability.feature_importance.feature_impoReg`` function. For illustration purpose, we run 3 repeats with 70%/30% train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "known-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n",
      "use coefs as feature importance \n",
      "1\n",
      "Iter:  1\n",
      "use coefs as feature importance \n",
      "2\n",
      "Iter:  2\n",
      "use coefs as feature importance \n"
     ]
    }
   ],
   "source": [
    "model_reg = imlreliability.feature_importance.feature_impoReg(data_reg,estimator=estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-federation",
   "metadata": {},
   "source": [
    "The ``.get_consistency`` function results in three pandas dataframe: ``accuracy``: prediction accuracy on test set; ``consistency``: interpretation consistency measured by RBO, Jaccard score, or user-defined metrics; and prediction_consistency measured by prediction entropy and purity if ``get_prediction_consistency ==True``. \n",
    "\n",
    "The ``consistency`` pandas dataframe can be downloaded and upload to the dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sought-province",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Function is  Coef_Ridge\n",
      "          data  model  Accuracy\n",
      "0  communities  Ridge  0.364518\n",
      "1  communities  Ridge  0.435435\n",
      "2  communities  Ridge  0.356309\n",
      "           data      method criteria   K  Consistency  Accuracy\n",
      "0   communities  Coef_Ridge      RBO   1        1.000     0.385\n",
      "1   communities  Coef_Ridge      RBO   2        0.875     0.385\n",
      "2   communities  Coef_Ridge      RBO   3        0.806     0.385\n",
      "3   communities  Coef_Ridge      RBO   4        0.792     0.385\n",
      "4   communities  Coef_Ridge      RBO   5        0.753     0.385\n",
      "5   communities  Coef_Ridge      RBO   6        0.725     0.385\n",
      "6   communities  Coef_Ridge      RBO   7        0.713     0.385\n",
      "7   communities  Coef_Ridge      RBO   8        0.702     0.385\n",
      "8   communities  Coef_Ridge      RBO   9        0.698     0.385\n",
      "9   communities  Coef_Ridge      RBO  10        0.698     0.385\n",
      "10  communities  Coef_Ridge      RBO  11        0.697     0.385\n",
      "11  communities  Coef_Ridge      RBO  12        0.698     0.385\n",
      "12  communities  Coef_Ridge      RBO  13        0.695     0.385\n",
      "13  communities  Coef_Ridge      RBO  14        0.691     0.385\n",
      "14  communities  Coef_Ridge      RBO  15        0.685     0.385\n",
      "15  communities  Coef_Ridge      RBO  16        0.679     0.385\n",
      "16  communities  Coef_Ridge      RBO  17        0.675     0.385\n",
      "17  communities  Coef_Ridge      RBO  18        0.672     0.385\n",
      "18  communities  Coef_Ridge      RBO  19        0.670     0.385\n",
      "19  communities  Coef_Ridge      RBO  20        0.669     0.385\n",
      "20  communities  Coef_Ridge      RBO  21        0.669     0.385\n",
      "21  communities  Coef_Ridge      RBO  22        0.670     0.385\n",
      "22  communities  Coef_Ridge      RBO  23        0.673     0.385\n",
      "23  communities  Coef_Ridge      RBO  24        0.675     0.385\n",
      "24  communities  Coef_Ridge      RBO  25        0.676     0.385\n",
      "25  communities  Coef_Ridge      RBO  26        0.678     0.385\n",
      "26  communities  Coef_Ridge      RBO  27        0.680     0.385\n",
      "27  communities  Coef_Ridge      RBO  28        0.682     0.385\n",
      "28  communities  Coef_Ridge      RBO  29        0.685     0.385\n",
      "29  communities  Coef_Ridge      RBO  30        0.687     0.385\n",
      "30  communities  Coef_Ridge  Jaccard   1        1.000     0.385\n",
      "31  communities  Coef_Ridge  Jaccard   2        0.667     0.385\n",
      "32  communities  Coef_Ridge  Jaccard   3        0.500     0.385\n",
      "33  communities  Coef_Ridge  Jaccard   4        0.667     0.385\n",
      "34  communities  Coef_Ridge  Jaccard   5        0.458     0.385\n",
      "35  communities  Coef_Ridge  Jaccard   6        0.417     0.385\n",
      "36  communities  Coef_Ridge  Jaccard   7        0.511     0.385\n",
      "37  communities  Coef_Ridge  Jaccard   8        0.504     0.385\n",
      "38  communities  Coef_Ridge  Jaccard   9        0.543     0.385\n",
      "39  communities  Coef_Ridge  Jaccard  10        0.576     0.385\n",
      "40  communities  Coef_Ridge  Jaccard  11        0.534     0.385\n",
      "41  communities  Coef_Ridge  Jaccard  12        0.590     0.385\n",
      "42  communities  Coef_Ridge  Jaccard  13        0.517     0.385\n",
      "43  communities  Coef_Ridge  Jaccard  14        0.511     0.385\n",
      "44  communities  Coef_Ridge  Jaccard  15        0.458     0.385\n",
      "45  communities  Coef_Ridge  Jaccard  16        0.440     0.385\n",
      "46  communities  Coef_Ridge  Jaccard  17        0.480     0.385\n",
      "47  communities  Coef_Ridge  Jaccard  18        0.461     0.385\n",
      "48  communities  Coef_Ridge  Jaccard  19        0.470     0.385\n",
      "49  communities  Coef_Ridge  Jaccard  20        0.490     0.385\n",
      "50  communities  Coef_Ridge  Jaccard  21        0.508     0.385\n",
      "51  communities  Coef_Ridge  Jaccard  22        0.548     0.385\n",
      "52  communities  Coef_Ridge  Jaccard  23        0.588     0.385\n",
      "53  communities  Coef_Ridge  Jaccard  24        0.550     0.385\n",
      "54  communities  Coef_Ridge  Jaccard  25        0.542     0.385\n",
      "55  communities  Coef_Ridge  Jaccard  26        0.577     0.385\n",
      "56  communities  Coef_Ridge  Jaccard  27        0.588     0.385\n",
      "57  communities  Coef_Ridge  Jaccard  28        0.578     0.385\n",
      "58  communities  Coef_Ridge  Jaccard  29        0.612     0.385\n",
      "59  communities  Coef_Ridge  Jaccard  30        0.623     0.385\n",
      "          data  model   Entropy    Purity\n",
      "0  communities  Ridge  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "model_reg.get_consistency(data_name='communities', estimator_name='Ridge',impotance_func_name='Coef')\n",
    "print(model_reg.accuracy)\n",
    "print(model_reg.consistency)\n",
    "print(model_reg.prediction_consistency)\n",
    "\n",
    "## model_reg.consistency.to_csv('consis_test_fi_reg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cbfa8",
   "metadata": {},
   "source": [
    "#### 1.1.2. Tree-base model\n",
    "Here we aim to evaluate the interpretation reliability of random forest, using the ``feature_impoReg``function.. We use ``RandomForestRegressor()`` from ``sklearn`` as our estimator. By setting ``importance_func=None``, the default feature importance ``feature_importances_`` of the ``RandomForestRegressor()`` function will be used to evaluate feature importance. \n",
    "All other settings are the same as linear regression in 1.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628a2df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n",
      "use feature_importances_ as feature importance \n",
      "1\n",
      "Iter:  1\n",
      "use feature_importances_ as feature importance \n",
      "2\n",
      "Iter:  2\n",
      "use feature_importances_ as feature importance \n",
      "Importance Function is  FI_RF\n",
      "          data model  Accuracy\n",
      "0  communities    RF  0.347407\n",
      "1  communities    RF  0.398921\n",
      "2  communities    RF  0.360784\n",
      "           data method criteria   K  Consistency  Accuracy\n",
      "0   communities  FI_RF      RBO   1        1.000     0.369\n",
      "1   communities  FI_RF      RBO   2        1.000     0.369\n",
      "2   communities  FI_RF      RBO   3        1.000     0.369\n",
      "3   communities  FI_RF      RBO   4        0.938     0.369\n",
      "4   communities  FI_RF      RBO   5        0.890     0.369\n",
      "5   communities  FI_RF      RBO   6        0.894     0.369\n",
      "6   communities  FI_RF      RBO   7        0.879     0.369\n",
      "7   communities  FI_RF      RBO   8        0.855     0.369\n",
      "8   communities  FI_RF      RBO   9        0.840     0.369\n",
      "9   communities  FI_RF      RBO  10        0.831     0.369\n",
      "10  communities  FI_RF      RBO  11        0.818     0.369\n",
      "11  communities  FI_RF      RBO  12        0.812     0.369\n",
      "12  communities  FI_RF      RBO  13        0.809     0.369\n",
      "13  communities  FI_RF      RBO  14        0.807     0.369\n",
      "14  communities  FI_RF      RBO  15        0.807     0.369\n",
      "15  communities  FI_RF      RBO  16        0.807     0.369\n",
      "16  communities  FI_RF      RBO  17        0.806     0.369\n",
      "17  communities  FI_RF      RBO  18        0.809     0.369\n",
      "18  communities  FI_RF      RBO  19        0.810     0.369\n",
      "19  communities  FI_RF      RBO  20        0.809     0.369\n",
      "20  communities  FI_RF      RBO  21        0.809     0.369\n",
      "21  communities  FI_RF      RBO  22        0.809     0.369\n",
      "22  communities  FI_RF      RBO  23        0.808     0.369\n",
      "23  communities  FI_RF      RBO  24        0.809     0.369\n",
      "24  communities  FI_RF      RBO  25        0.811     0.369\n",
      "25  communities  FI_RF      RBO  26        0.813     0.369\n",
      "26  communities  FI_RF      RBO  27        0.814     0.369\n",
      "27  communities  FI_RF      RBO  28        0.815     0.369\n",
      "28  communities  FI_RF      RBO  29        0.815     0.369\n",
      "29  communities  FI_RF      RBO  30        0.815     0.369\n",
      "30  communities  FI_RF  Jaccard   1        1.000     0.369\n",
      "31  communities  FI_RF  Jaccard   2        1.000     0.369\n",
      "32  communities  FI_RF  Jaccard   3        1.000     0.369\n",
      "33  communities  FI_RF  Jaccard   4        0.600     0.369\n",
      "34  communities  FI_RF  Jaccard   5        0.548     0.369\n",
      "35  communities  FI_RF  Jaccard   6        0.857     0.369\n",
      "36  communities  FI_RF  Jaccard   7        0.653     0.369\n",
      "37  communities  FI_RF  Jaccard   8        0.527     0.369\n",
      "38  communities  FI_RF  Jaccard   9        0.568     0.369\n",
      "39  communities  FI_RF  Jaccard  10        0.603     0.369\n",
      "40  communities  FI_RF  Jaccard  11        0.519     0.369\n",
      "41  communities  FI_RF  Jaccard  12        0.600     0.369\n",
      "42  communities  FI_RF  Jaccard  13        0.631     0.369\n",
      "43  communities  FI_RF  Jaccard  14        0.647     0.369\n",
      "44  communities  FI_RF  Jaccard  15        0.667     0.369\n",
      "45  communities  FI_RF  Jaccard  16        0.684     0.369\n",
      "46  communities  FI_RF  Jaccard  17        0.660     0.369\n",
      "47  communities  FI_RF  Jaccard  18        0.757     0.369\n",
      "48  communities  FI_RF  Jaccard  19        0.690     0.369\n",
      "49  communities  FI_RF  Jaccard  20        0.670     0.369\n",
      "50  communities  FI_RF  Jaccard  21        0.683     0.369\n",
      "51  communities  FI_RF  Jaccard  22        0.661     0.369\n",
      "52  communities  FI_RF  Jaccard  23        0.673     0.369\n",
      "53  communities  FI_RF  Jaccard  24        0.685     0.369\n",
      "54  communities  FI_RF  Jaccard  25        0.755     0.369\n",
      "55  communities  FI_RF  Jaccard  26        0.763     0.369\n",
      "56  communities  FI_RF  Jaccard  27        0.742     0.369\n",
      "57  communities  FI_RF  Jaccard  28        0.723     0.369\n",
      "58  communities  FI_RF  Jaccard  29        0.707     0.369\n",
      "59  communities  FI_RF  Jaccard  30        0.690     0.369\n",
      "          data model   Entropy    Purity\n",
      "0  communities    RF  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "estimator=RandomForestRegressor()\n",
    "importance_func=None\n",
    "\n",
    "\n",
    "model_reg_tree=imlreliability.feature_importance.feature_impoReg(data_reg,estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg_tree.fit()\n",
    "model_reg_tree.get_consistency(data_name='communities', estimator_name='RF',impotance_func_name='FI')\n",
    "print(model_reg_tree.accuracy)\n",
    "print(model_reg_tree.consistency)\n",
    "print(model_reg_tree.prediction_consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8dcc1e",
   "metadata": {},
   "source": [
    "### 1.2. Model agnostic \n",
    "For model agnostic methods to measure feature importance, we provide built-in importance functions from package shap and perumutation function from sklearn.inspection. The imlreliability also support elf-defined importance function, with three argument: ``(fitted model, training x, training y)``, and 1 output importance score in forms of list or array:\n",
    "\n",
    "``importance_func(self.fitted,x_train, y_train)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bad43b",
   "metadata": {},
   "source": [
    "#### 1.2.1. Permutation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-specific",
   "metadata": {},
   "source": [
    "##### 1.2.1.1. Random Forest + Permutation\n",
    "Here we use random forest to consturct the prediction model using the ``feature_impoReg``function, and permutation as the post-hoc method to measure the feature importance, by setting ``importance_func=permutation_importance``. All other settings are the same as linear regression in 1.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06725fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n",
      "1\n",
      "Iter:  1\n",
      "2\n",
      "Iter:  2\n",
      "Importance Function is  Permutation_RF\n",
      "          data model  Accuracy\n",
      "0  communities    RF  0.335631\n",
      "1  communities    RF  0.397873\n",
      "2  communities    RF  0.353835\n",
      "           data          method criteria   K  Consistency  Accuracy\n",
      "0   communities  Permutation_RF      RBO   1        1.000     0.362\n",
      "1   communities  Permutation_RF      RBO   2        1.000     0.362\n",
      "2   communities  Permutation_RF      RBO   3        1.000     0.362\n",
      "3   communities  Permutation_RF      RBO   4        0.969     0.362\n",
      "4   communities  Permutation_RF      RBO   5        0.935     0.362\n",
      "5   communities  Permutation_RF      RBO   6        0.932     0.362\n",
      "6   communities  Permutation_RF      RBO   7        0.921     0.362\n",
      "7   communities  Permutation_RF      RBO   8        0.900     0.362\n",
      "8   communities  Permutation_RF      RBO   9        0.886     0.362\n",
      "9   communities  Permutation_RF      RBO  10        0.868     0.362\n",
      "10  communities  Permutation_RF      RBO  11        0.859     0.362\n",
      "11  communities  Permutation_RF      RBO  12        0.850     0.362\n",
      "12  communities  Permutation_RF      RBO  13        0.847     0.362\n",
      "13  communities  Permutation_RF      RBO  14        0.845     0.362\n",
      "14  communities  Permutation_RF      RBO  15        0.842     0.362\n",
      "15  communities  Permutation_RF      RBO  16        0.838     0.362\n",
      "16  communities  Permutation_RF      RBO  17        0.836     0.362\n",
      "17  communities  Permutation_RF      RBO  18        0.834     0.362\n",
      "18  communities  Permutation_RF      RBO  19        0.832     0.362\n",
      "19  communities  Permutation_RF      RBO  20        0.830     0.362\n",
      "20  communities  Permutation_RF      RBO  21        0.829     0.362\n",
      "21  communities  Permutation_RF      RBO  22        0.831     0.362\n",
      "22  communities  Permutation_RF      RBO  23        0.831     0.362\n",
      "23  communities  Permutation_RF      RBO  24        0.831     0.362\n",
      "24  communities  Permutation_RF      RBO  25        0.831     0.362\n",
      "25  communities  Permutation_RF      RBO  26        0.830     0.362\n",
      "26  communities  Permutation_RF      RBO  27        0.829     0.362\n",
      "27  communities  Permutation_RF      RBO  28        0.829     0.362\n",
      "28  communities  Permutation_RF      RBO  29        0.827     0.362\n",
      "29  communities  Permutation_RF      RBO  30        0.826     0.362\n",
      "30  communities  Permutation_RF  Jaccard   1        1.000     0.362\n",
      "31  communities  Permutation_RF  Jaccard   2        1.000     0.362\n",
      "32  communities  Permutation_RF  Jaccard   3        1.000     0.362\n",
      "33  communities  Permutation_RF  Jaccard   4        0.800     0.362\n",
      "34  communities  Permutation_RF  Jaccard   5        0.667     0.362\n",
      "35  communities  Permutation_RF  Jaccard   6        0.857     0.362\n",
      "36  communities  Permutation_RF  Jaccard   7        0.750     0.362\n",
      "37  communities  Permutation_RF  Jaccard   8        0.600     0.362\n",
      "38  communities  Permutation_RF  Jaccard   9        0.636     0.362\n",
      "39  communities  Permutation_RF  Jaccard  10        0.538     0.362\n",
      "40  communities  Permutation_RF  Jaccard  11        0.632     0.362\n",
      "41  communities  Permutation_RF  Jaccard  12        0.600     0.362\n",
      "42  communities  Permutation_RF  Jaccard  13        0.679     0.362\n",
      "43  communities  Permutation_RF  Jaccard  14        0.699     0.362\n",
      "44  communities  Permutation_RF  Jaccard  15        0.672     0.362\n",
      "45  communities  Permutation_RF  Jaccard  16        0.642     0.362\n",
      "46  communities  Permutation_RF  Jaccard  17        0.660     0.362\n",
      "47  communities  Permutation_RF  Jaccard  18        0.675     0.362\n",
      "48  communities  Permutation_RF  Jaccard  19        0.652     0.362\n",
      "49  communities  Permutation_RF  Jaccard  20        0.670     0.362\n",
      "50  communities  Permutation_RF  Jaccard  21        0.683     0.362\n",
      "51  communities  Permutation_RF  Jaccard  22        0.763     0.362\n",
      "52  communities  Permutation_RF  Jaccard  23        0.736     0.362\n",
      "53  communities  Permutation_RF  Jaccard  24        0.716     0.362\n",
      "54  communities  Permutation_RF  Jaccard  25        0.699     0.362\n",
      "55  communities  Permutation_RF  Jaccard  26        0.679     0.362\n",
      "56  communities  Permutation_RF  Jaccard  27        0.689     0.362\n",
      "57  communities  Permutation_RF  Jaccard  28        0.675     0.362\n",
      "58  communities  Permutation_RF  Jaccard  29        0.658     0.362\n",
      "59  communities  Permutation_RF  Jaccard  30        0.668     0.362\n",
      "          data model   Entropy    Purity\n",
      "0  communities    RF  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "estimator=RandomForestRegressor()\n",
    "importance_func = permutation_importance ## change the importance function to be permutation \n",
    "\n",
    "\n",
    "model_reg_tree_per=imlreliability.feature_importance.feature_impoReg(data_reg,estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg_tree_per.fit()\n",
    "model_reg_tree_per.get_consistency(data_name='communities', estimator_name='RF',impotance_func_name='Permutation')\n",
    "print(model_reg_tree_per.accuracy)\n",
    "print(model_reg_tree_per.consistency)\n",
    "print(model_reg_tree_per.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-probability",
   "metadata": {},
   "source": [
    "##### 1.2.1.2. MLP + Permutation\n",
    "Here we construct a MLP model with two hidden layers as the prediction model, and permutation as the post-hoc method to measure the feature importance, by setting ``importance_func=permutation_importance``. Note that here we use the ``feature_impoReg_MLP`` function for MLP-based techniques. All other settings are the same as linear regression in 1.1.1. \n",
    "\n",
    "A two-layer default MLP will be computed if ``estimator = None``. We also support user-defined MLP models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "simple-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Iter:  0\n",
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "['eli5', 'sklearn', 'permutation_importance']\n",
      "598/598 [==============================] - 0s 258us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4585\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4700\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4645\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4581\n",
      "598/598 [==============================] - 0s 66us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4610\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4503\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4535\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4586\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4548\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4545\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 85us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.4601\n",
      "598/598 [==============================] - 0s 53us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 264us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 64us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 77us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4535\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4573\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4567\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4520\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.4516\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4579\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 180us/sample - loss: 0.4576\n",
      "598/598 [==============================] - 0s 61us/sample - loss: 0.4662\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4548\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.4633\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4627\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4647\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4533\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4517\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 66us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4585\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 123us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 58us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 65us/sample - loss: 0.4545\n",
      "598/598 [==============================] - 0s 58us/sample - loss: 0.4588\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4519\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4504\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4584\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4576\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4527\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4579\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4539\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4579\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4545\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4620\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4621\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4571\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4571\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4582\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4613\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4728\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4634\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4588\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4585\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.4519\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4590\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4577\n",
      "598/598 [==============================] - 0s 95us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4607\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4521\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4579\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4527\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4529\n",
      "598/598 [==============================] - 0s 69us/sample - loss: 0.4573\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4543\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4537\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4571\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4608\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4620\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4562\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4638\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4629\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4598\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4500\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4537\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 74us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4577\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4571\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 73us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4534\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4573\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4545\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4511\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4576\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4586\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4517\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4532\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4534\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4572\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4613\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4632\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4590\n",
      "598/598 [==============================] - 0s 71us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 65us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4580\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4578\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4697\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4623\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4581\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4562\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4593\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4521\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4529\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4584\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 59us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4609\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4512\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4519\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4534\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4578\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4572\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4526\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4529\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4597\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4587\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4614\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 64us/sample - loss: 0.4523\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4606\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4635\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4699\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4585\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.4543\n",
      "598/598 [==============================] - 0s 65us/sample - loss: 0.4537\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4562\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4579\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.4520\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4581\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4531\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4504\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4591\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4596\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4529\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4548\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4594\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.4572\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4578\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4559\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4573\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4538\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.4617\n",
      "598/598 [==============================] - 0s 80us/sample - loss: 0.4644\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 62us/sample - loss: 0.4585\n",
      "598/598 [==============================] - 0s 67us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4562\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4569\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4603\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4644\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4637\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4586\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4583\n",
      "598/598 [==============================] - 0s 129us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4600\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4509\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4573\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4567\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4584\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4583\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4537\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.4505\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4532\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4516\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4541\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4596\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4535\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4588\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4601\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4533\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4629\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4634\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4644\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4589\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4529\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4537\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4573\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4543\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4582\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4500\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4588\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4532\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4545\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4539\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4567\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4572\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4568\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4614\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4636\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4603\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4567\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4555\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4562\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4594\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4691\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4625\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4576\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 22us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4587\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4564\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4503\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4532\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4594\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4605\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4553\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4571\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4556\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4527\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4560\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4534\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 63us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4597\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4520\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4545\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4551\n",
      "598/598 [==============================] - 0s 77us/sample - loss: 0.4535\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.4517\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4578\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4531\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4591\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 75us/sample - loss: 0.4592\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4622\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4571\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4548\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4620\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4685\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4584\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4514\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4554\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4584\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4529\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4546\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4552\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4540\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4530\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4597\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4544\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4520\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4578\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4567\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4539\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4561\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4569\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4563\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4549\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4536\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4562\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4578\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4565\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4543\n",
      "598/598 [==============================] - 0s 60us/sample - loss: 0.4542\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4547\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4623\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4653\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4566\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4575\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4557\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4558\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4570\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.4572\n",
      "598/598 [==============================] - 0s 61us/sample - loss: 0.4550\n",
      "598/598 [==============================] - 0s 55us/sample - loss: 0.4584\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4574\n",
      "598/598 [==============================] - 0s 189us/sample - loss: 0.5119\n",
      "Iter:  1\n",
      "['eli5', 'sklearn', 'permutation_importance']\n",
      "598/598 [==============================] - 0s 222us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5141\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5321\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5166\n",
      "598/598 [==============================] - 0s 55us/sample - loss: 0.5145\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 90us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 78us/sample - loss: 0.5084\n",
      "598/598 [==============================] - 0s 83us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5152\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5072\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5153\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5089\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5061\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5099\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5107\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5147\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5071\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5029\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5096\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5056\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5145\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 67us/sample - loss: 0.5088\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5149\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5190\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5089\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5153\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5165\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5194\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5129\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5141\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5136\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 66us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5039\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5099\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5164\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5095\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5099\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5139\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5062\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5088\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5143\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5169\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5148\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5238\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5142\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5144\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5225\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5162\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5091\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5088\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5081\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 90us/sample - loss: 0.5146\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5067\n",
      "598/598 [==============================] - 0s 77us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5074\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5141\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5123\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5115\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5079\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5070\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5069\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5060\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5150\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5149\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5054\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5098\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.5096\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5182\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5194\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5095\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5136\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5136\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5043\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 240us/sample - loss: 0.5094\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5091\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5175\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5074\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5092\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5040\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5099\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5097\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5087\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5146\n",
      "598/598 [==============================] - 0s 58us/sample - loss: 0.5135\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5129\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5102\n",
      "598/598 [==============================] - 0s 65us/sample - loss: 0.5159\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5145\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5200\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5142\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5135\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5313\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5162\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5145\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5139\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5059\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5107\n",
      "598/598 [==============================] - 0s 22us/sample - loss: 0.5174\n",
      "598/598 [==============================] - 0s 21us/sample - loss: 0.5069\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5075\n",
      "598/598 [==============================] - 0s 22us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5091\n",
      "598/598 [==============================] - 0s 21us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5097\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5074\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5081\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5137\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5068\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5146\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5099\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5173\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5112\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5090\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5112\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5198\n",
      "598/598 [==============================] - 0s 77us/sample - loss: 0.5233\n",
      "598/598 [==============================] - 0s 53us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5094\n",
      "598/598 [==============================] - 0s 102us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5112\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.5052\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5123\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5137\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5119\n",
      "598/598 [==============================] - ETA: 0s - loss: 0.381 - 0s 59us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5027\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5094\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5068\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5179\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 214us/sample - loss: 0.5075\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.5036\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5115\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5107\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5123\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 59us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5154\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5145\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5193\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5106\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5135\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5236\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5199\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5143\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5143\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5107\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 128us/sample - loss: 0.5130s - loss: 0.514\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5074\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5150\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5058\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5062\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 65us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5076\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5058\n",
      "598/598 [==============================] - ETA: 0s - loss: 0.392 - 0s 46us/sample - loss: 0.5095\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5139\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5015\n",
      "598/598 [==============================] - 0s 53us/sample - loss: 0.5129\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5129\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5106\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.5162\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5137\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 198us/sample - loss: 0.5148\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5167\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5123\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5089\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5158\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5115\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5192\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5196\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5093\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5103\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5032\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5137\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 81us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5055\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5151\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5160\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5090\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5087\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5090\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5117\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5114\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5095\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.5107\n",
      "598/598 [==============================] - 0s 55us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5148\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5144\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5121\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5139\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5232\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5092\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5204\n",
      "598/598 [==============================] - 0s 58us/sample - loss: 0.5218\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.5142\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5084\n",
      "598/598 [==============================] - 0s 48us/sample - loss: 0.5094\n",
      "598/598 [==============================] - 0s 71us/sample - loss: 0.5147\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5123\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5059\n",
      "598/598 [==============================] - 0s 53us/sample - loss: 0.5123\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5130\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5086\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5153\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.5102\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5125\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5101\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5106\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5116\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5063\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.5073\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5041\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5045\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5141\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5136\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5096\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5108\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5132\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.5101\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5150\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.5110\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5142\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5144\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.5127\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5182\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.5174\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.5119\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5097\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5113\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5066\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5128\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.5131\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5139\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5141\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.5139\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.5156\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5133\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.5122\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.5104\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5054\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5093\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5095\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.5096\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.5158\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.5052\n",
      "598/598 [==============================] - 0s 121us/sample - loss: 0.5101\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5129\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.5080\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5100\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.5101\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5120\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5115\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5089\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.5111\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.5109\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5156\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5135\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5134\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.5118\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5129\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.5105\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.5158\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.5124\n",
      "598/598 [==============================] - 0s 204us/sample - loss: 0.5140\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.5138\n",
      "598/598 [==============================] - 0s 82us/sample - loss: 0.5158\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.5126\n",
      "598/598 [==============================] - 0s 84us/sample - loss: 0.4468\n",
      "Iter:  2\n",
      "['eli5', 'sklearn', 'permutation_importance']\n",
      "598/598 [==============================] - 0s 169us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4795\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4821\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4829\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4780\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4780\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4782\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4812\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4753\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4782\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 53us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4745\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4756\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4756\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4729\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4827\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4748\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4799\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 89us/sample - loss: 0.4701\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4796\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4855\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4820\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4799\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4796\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4870\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4833\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4884\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4813\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4748\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4747\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4747\n",
      "598/598 [==============================] - 0s 50us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 108us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 44us/sample - loss: 0.4773\n",
      "598/598 [==============================] - ETA: 0s - loss: 0.396 - 0s 44us/sample - loss: 0.4793\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4740\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 54us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 89us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4858\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4816\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 314us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4747\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4780\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 58us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4796\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4802\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4800\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4783\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 90us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4806\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 52us/sample - loss: 0.4812\n",
      "598/598 [==============================] - 0s 156us/sample - loss: 0.4864\n",
      "598/598 [==============================] - 0s 53us/sample - loss: 0.4754\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4738\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4745\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4789\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4821\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 0s 73us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 250us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 57us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 49us/sample - loss: 0.4744\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 96us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4743\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4756\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 55us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 56us/sample - loss: 0.4755\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4739\n",
      "598/598 [==============================] - 0s 76us/sample - loss: 0.4809\n",
      "598/598 [==============================] - 0s 43us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4747\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4788\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4753\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4755\n",
      "598/598 [==============================] - 0s 60us/sample - loss: 0.4787\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 100us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4788\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4811\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4818\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4858\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4824\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4885\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4822\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4748\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4780\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4727\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4754\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4740\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4792\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4826\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4794\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4819\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4754\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4832\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4795\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4785\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4827\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4785\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4787\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4796\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4809\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4865\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4740\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4748\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4789\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 22us/sample - loss: 0.4798\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4743\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4735\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4755\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4754\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4715\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4814\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4744\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4754\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4785\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4753\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4793\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4743\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4796\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4843\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4793\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4786\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4816\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4813\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4799\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4821\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4843\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4880\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4783\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4755\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4749\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4743\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 67us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 47us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 61us/sample - loss: 0.4832\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4780\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4798\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4747\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4735\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4750\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4756\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4736\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4819\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4800\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4785\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4803\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4792\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4797\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4820\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4847\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 46us/sample - loss: 0.4788\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4802\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4756\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4732\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4824\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4734\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4753\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4734\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4794\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4831\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4799\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4784\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4828\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4827\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4826\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4868\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4797\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4748\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4754\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4768\n",
      "598/598 [==============================] - 0s 22us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4814\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4834\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4821\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4789\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4756\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4746\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4820\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4798\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4783\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4783\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4805\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4784\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4795\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4793\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4821\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4880\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4757\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4735\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4753\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4782\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4786\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4795\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 40us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4779\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4740\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 41us/sample - loss: 0.4728\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4801\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4734\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4765\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4742\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4790\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4734\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4751\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4784\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4799\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4817\n",
      "598/598 [==============================] - 0s 34us/sample - loss: 0.4795\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4798\n",
      "598/598 [==============================] - 0s 38us/sample - loss: 0.4821\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4792\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4823\n",
      "598/598 [==============================] - 0s 60us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4844\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4882\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4798\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4763\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 24us/sample - loss: 0.4764\n",
      "598/598 [==============================] - 0s 23us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4741\n",
      "598/598 [==============================] - 0s 25us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4770\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4749\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4784\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4769\n",
      "598/598 [==============================] - 0s 27us/sample - loss: 0.4762\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4748\n",
      "598/598 [==============================] - 0s 36us/sample - loss: 0.4781\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4773\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4759\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4840\n",
      "598/598 [==============================] - 0s 45us/sample - loss: 0.4796\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4775\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4801\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4755\n",
      "598/598 [==============================] - 0s 72us/sample - loss: 0.4758\n",
      "598/598 [==============================] - 0s 26us/sample - loss: 0.4767\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4760\n",
      "598/598 [==============================] - 0s 30us/sample - loss: 0.4752\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4772\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4766\n",
      "598/598 [==============================] - 0s 33us/sample - loss: 0.4753\n",
      "598/598 [==============================] - 0s 28us/sample - loss: 0.4735\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 39us/sample - loss: 0.4800\n",
      "598/598 [==============================] - 0s 35us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4771\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4790\n",
      "598/598 [==============================] - 0s 37us/sample - loss: 0.4790\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4778\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4777\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4761\n",
      "598/598 [==============================] - 0s 31us/sample - loss: 0.4811\n",
      "598/598 [==============================] - 0s 32us/sample - loss: 0.4784\n",
      "598/598 [==============================] - 0s 42us/sample - loss: 0.4785\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4774\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4782\n",
      "598/598 [==============================] - 0s 29us/sample - loss: 0.4776\n",
      "598/598 [==============================] - 0s 67us/sample - loss: 0.3376\n",
      "Importance Function is  Permutation_MLP\n",
      "          data model  Accuracy\n",
      "0  communities   MLP  0.511927\n",
      "1  communities   MLP  0.446814\n",
      "2  communities   MLP  0.337561\n",
      "           data           method criteria   K  Consistency  Accuracy\n",
      "0   communities  Permutation_MLP      RBO   1        0.500     0.432\n",
      "1   communities  Permutation_MLP      RBO   2        0.500     0.432\n",
      "2   communities  Permutation_MLP      RBO   3        0.444     0.432\n",
      "3   communities  Permutation_MLP      RBO   4        0.427     0.432\n",
      "4   communities  Permutation_MLP      RBO   5        0.442     0.432\n",
      "5   communities  Permutation_MLP      RBO   6        0.451     0.432\n",
      "6   communities  Permutation_MLP      RBO   7        0.458     0.432\n",
      "7   communities  Permutation_MLP      RBO   8        0.464     0.432\n",
      "8   communities  Permutation_MLP      RBO   9        0.468     0.432\n",
      "9   communities  Permutation_MLP      RBO  10        0.476     0.432\n",
      "10  communities  Permutation_MLP      RBO  11        0.482     0.432\n",
      "11  communities  Permutation_MLP      RBO  12        0.484     0.432\n",
      "12  communities  Permutation_MLP      RBO  13        0.482     0.432\n",
      "13  communities  Permutation_MLP      RBO  14        0.478     0.432\n",
      "14  communities  Permutation_MLP      RBO  15        0.473     0.432\n",
      "15  communities  Permutation_MLP      RBO  16        0.469     0.432\n",
      "16  communities  Permutation_MLP      RBO  17        0.467     0.432\n",
      "17  communities  Permutation_MLP      RBO  18        0.466     0.432\n",
      "18  communities  Permutation_MLP      RBO  19        0.465     0.432\n",
      "19  communities  Permutation_MLP      RBO  20        0.464     0.432\n",
      "20  communities  Permutation_MLP      RBO  21        0.466     0.432\n",
      "21  communities  Permutation_MLP      RBO  22        0.467     0.432\n",
      "22  communities  Permutation_MLP      RBO  23        0.470     0.432\n",
      "23  communities  Permutation_MLP      RBO  24        0.471     0.432\n",
      "24  communities  Permutation_MLP      RBO  25        0.471     0.432\n",
      "25  communities  Permutation_MLP      RBO  26        0.471     0.432\n",
      "26  communities  Permutation_MLP      RBO  27        0.472     0.432\n",
      "27  communities  Permutation_MLP      RBO  28        0.474     0.432\n",
      "28  communities  Permutation_MLP      RBO  29        0.475     0.432\n",
      "29  communities  Permutation_MLP      RBO  30        0.475     0.432\n",
      "30  communities  Permutation_MLP  Jaccard   1        0.500     0.432\n",
      "31  communities  Permutation_MLP  Jaccard   2        0.333     0.432\n",
      "32  communities  Permutation_MLP  Jaccard   3        0.200     0.432\n",
      "33  communities  Permutation_MLP  Jaccard   4        0.238     0.432\n",
      "34  communities  Permutation_MLP  Jaccard   5        0.339     0.432\n",
      "35  communities  Permutation_MLP  Jaccard   6        0.350     0.432\n",
      "36  communities  Permutation_MLP  Jaccard   7        0.361     0.432\n",
      "37  communities  Permutation_MLP  Jaccard   8        0.343     0.432\n",
      "38  communities  Permutation_MLP  Jaccard   9        0.350     0.432\n",
      "39  communities  Permutation_MLP  Jaccard  10        0.381     0.432\n",
      "40  communities  Permutation_MLP  Jaccard  11        0.380     0.432\n",
      "41  communities  Permutation_MLP  Jaccard  12        0.337     0.432\n",
      "42  communities  Permutation_MLP  Jaccard  13        0.303     0.432\n",
      "43  communities  Permutation_MLP  Jaccard  14        0.275     0.432\n",
      "44  communities  Permutation_MLP  Jaccard  15        0.252     0.432\n",
      "45  communities  Permutation_MLP  Jaccard  16        0.255     0.432\n",
      "46  communities  Permutation_MLP  Jaccard  17        0.287     0.432\n",
      "47  communities  Permutation_MLP  Jaccard  18        0.292     0.432\n",
      "48  communities  Permutation_MLP  Jaccard  19        0.297     0.432\n",
      "49  communities  Permutation_MLP  Jaccard  20        0.296     0.432\n",
      "50  communities  Permutation_MLP  Jaccard  21        0.342     0.432\n",
      "51  communities  Permutation_MLP  Jaccard  22        0.344     0.432\n",
      "52  communities  Permutation_MLP  Jaccard  23        0.364     0.432\n",
      "53  communities  Permutation_MLP  Jaccard  24        0.343     0.432\n",
      "54  communities  Permutation_MLP  Jaccard  25        0.324     0.432\n",
      "55  communities  Permutation_MLP  Jaccard  26        0.307     0.432\n",
      "56  communities  Permutation_MLP  Jaccard  27        0.338     0.432\n",
      "57  communities  Permutation_MLP  Jaccard  28        0.351     0.432\n",
      "58  communities  Permutation_MLP  Jaccard  29        0.335     0.432\n",
      "59  communities  Permutation_MLP  Jaccard  30        0.334     0.432\n",
      "          data model   Entropy    Purity\n",
      "0  communities   MLP  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from eli5.sklearn import PermutationImportance\n",
    "importance_func =PermutationImportance\n",
    "\n",
    "model_reg_mlp_dl=imlreliability.feature_importance.feature_impoReg_MLP(data_reg,\n",
    "                 importance_func=PermutationImportance,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg_mlp_dl.fit()\n",
    "model_reg_mlp_dl.get_consistency(data_name='communities', estimator_name='MLP',impotance_func_name='Permutation')\n",
    "print(model_reg_mlp_dl.accuracy)\n",
    "print(model_reg_mlp_dl.consistency)\n",
    "print(model_reg_mlp_dl.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce05eb",
   "metadata": {},
   "source": [
    "#### 1.2.2. Shapley Value \n",
    "\n",
    "Here we use random forest to consturct the prediction model, and SHAP as the post-hoc method to measure the feature importance, by setting ``importance_func=shap.TreeExplainer``. All other settings are the same as linear regression in 1.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "covered-ferry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n",
      "1\n",
      "Iter:  1\n",
      "2\n",
      "Iter:  2\n",
      "Importance Function is  SHAP_RF\n",
      "          data model  Accuracy\n",
      "0  communities    RF  0.348602\n",
      "1  communities    RF  0.397247\n",
      "2  communities    RF  0.361508\n",
      "           data   method criteria   K  Consistency  Accuracy\n",
      "0   communities  SHAP_RF      RBO   1        1.000     0.369\n",
      "1   communities  SHAP_RF      RBO   2        0.750     0.369\n",
      "2   communities  SHAP_RF      RBO   3        0.722     0.369\n",
      "3   communities  SHAP_RF      RBO   4        0.698     0.369\n",
      "4   communities  SHAP_RF      RBO   5        0.678     0.369\n",
      "5   communities  SHAP_RF      RBO   6        0.662     0.369\n",
      "6   communities  SHAP_RF      RBO   7        0.649     0.369\n",
      "7   communities  SHAP_RF      RBO   8        0.631     0.369\n",
      "8   communities  SHAP_RF      RBO   9        0.622     0.369\n",
      "9   communities  SHAP_RF      RBO  10        0.615     0.369\n",
      "10  communities  SHAP_RF      RBO  11        0.609     0.369\n",
      "11  communities  SHAP_RF      RBO  12        0.607     0.369\n",
      "12  communities  SHAP_RF      RBO  13        0.607     0.369\n",
      "13  communities  SHAP_RF      RBO  14        0.605     0.369\n",
      "14  communities  SHAP_RF      RBO  15        0.600     0.369\n",
      "15  communities  SHAP_RF      RBO  16        0.596     0.369\n",
      "16  communities  SHAP_RF      RBO  17        0.592     0.369\n",
      "17  communities  SHAP_RF      RBO  18        0.587     0.369\n",
      "18  communities  SHAP_RF      RBO  19        0.581     0.369\n",
      "19  communities  SHAP_RF      RBO  20        0.577     0.369\n",
      "20  communities  SHAP_RF      RBO  21        0.574     0.369\n",
      "21  communities  SHAP_RF      RBO  22        0.571     0.369\n",
      "22  communities  SHAP_RF      RBO  23        0.568     0.369\n",
      "23  communities  SHAP_RF      RBO  24        0.567     0.369\n",
      "24  communities  SHAP_RF      RBO  25        0.566     0.369\n",
      "25  communities  SHAP_RF      RBO  26        0.566     0.369\n",
      "26  communities  SHAP_RF      RBO  27        0.568     0.369\n",
      "27  communities  SHAP_RF      RBO  28        0.570     0.369\n",
      "28  communities  SHAP_RF      RBO  29        0.571     0.369\n",
      "29  communities  SHAP_RF      RBO  30        0.572     0.369\n",
      "30  communities  SHAP_RF  Jaccard   1        1.000     0.369\n",
      "31  communities  SHAP_RF  Jaccard   2        0.333     0.369\n",
      "32  communities  SHAP_RF  Jaccard   3        0.500     0.369\n",
      "33  communities  SHAP_RF  Jaccard   4        0.467     0.369\n",
      "34  communities  SHAP_RF  Jaccard   5        0.458     0.369\n",
      "35  communities  SHAP_RF  Jaccard   6        0.457     0.369\n",
      "36  communities  SHAP_RF  Jaccard   7        0.414     0.369\n",
      "37  communities  SHAP_RF  Jaccard   8        0.343     0.369\n",
      "38  communities  SHAP_RF  Jaccard   9        0.385     0.369\n",
      "39  communities  SHAP_RF  Jaccard  10        0.381     0.369\n",
      "40  communities  SHAP_RF  Jaccard  11        0.380     0.369\n",
      "41  communities  SHAP_RF  Jaccard  12        0.432     0.369\n",
      "42  communities  SHAP_RF  Jaccard  13        0.463     0.369\n",
      "43  communities  SHAP_RF  Jaccard  14        0.414     0.369\n",
      "44  communities  SHAP_RF  Jaccard  15        0.375     0.369\n",
      "45  communities  SHAP_RF  Jaccard  16        0.377     0.369\n",
      "46  communities  SHAP_RF  Jaccard  17        0.380     0.369\n",
      "47  communities  SHAP_RF  Jaccard  18        0.350     0.369\n",
      "48  communities  SHAP_RF  Jaccard  19        0.325     0.369\n",
      "49  communities  SHAP_RF  Jaccard  20        0.347     0.369\n",
      "50  communities  SHAP_RF  Jaccard  21        0.368     0.369\n",
      "51  communities  SHAP_RF  Jaccard  22        0.344     0.369\n",
      "52  communities  SHAP_RF  Jaccard  23        0.340     0.369\n",
      "53  communities  SHAP_RF  Jaccard  24        0.376     0.369\n",
      "54  communities  SHAP_RF  Jaccard  25        0.376     0.369\n",
      "55  communities  SHAP_RF  Jaccard  26        0.415     0.369\n",
      "56  communities  SHAP_RF  Jaccard  27        0.453     0.369\n",
      "57  communities  SHAP_RF  Jaccard  28        0.467     0.369\n",
      "58  communities  SHAP_RF  Jaccard  29        0.443     0.369\n",
      "59  communities  SHAP_RF  Jaccard  30        0.456     0.369\n",
      "          data model   Entropy    Purity\n",
      "0  communities    RF  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "estimator=RandomForestRegressor()\n",
    "importance_func = shap.TreeExplainer ## change the importance function to be SHAP \n",
    "\n",
    "model_reg_tree_shap=imlreliability.feature_importance.feature_impoReg(data_reg,estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg_tree_shap.fit()\n",
    "model_reg_tree_shap.get_consistency(data_name='communities', estimator_name='RF',impotance_func_name='SHAP')\n",
    "print(model_reg_tree_shap.accuracy)\n",
    "print(model_reg_tree_shap.consistency)\n",
    "print(model_reg_tree_shap.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523664c",
   "metadata": {},
   "source": [
    "### 1.3. MLP specific models \n",
    "We have built-in functions to run functions from ``deepexplain`` and  ``deeplift`` packages. As well permutation and shapley values. User can either input a function or a string from [\n",
    "                        'zero',\n",
    "                        'saliency',\n",
    "                        'grad*input',\n",
    "                        'intgrad',\n",
    "                        'elrp',\n",
    "                        'deeplift',\n",
    "                        'occlusion',\n",
    "                        'shapley_sampling'] to run deepExplain. \n",
    "Or input strings from ['NonlinearMxtsMode.RevealCancel','NonlinearMxtsMode.GuidedBackprop'...] to run corresponding functions in deeplift. \n",
    "\n",
    "\n",
    "imlreliability package also support self-defined importance function, with three argument: ``(fitted model, training x, training y)``, and 1 output importance score in forms of list or array:``importance_func(model,x_train, y_train)``. \n",
    "\n",
    "And the defined estimator needs to be form of :\n",
    "      \n",
    "```Python\n",
    "def _base_model_regression():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(M, input_dim=M, activation='relu'))\n",
    "    model.add(Dense(M, input_dim=M, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model  \n",
    "```\n",
    "\n",
    "And the trained MLP model is saved as .h5 file. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a78df",
   "metadata": {},
   "source": [
    "#### 1.3.1. Deeplift\n",
    "\n",
    "Here we construct a MLP model with two hidden layers as the prediction model, and deeplift as the post-hoc method to measure the feature importance, by setting ``importance_func='NonlinearMxtsMode.RevealCancel'``. All other settings are the same as linear regression in 1.1. \n",
    "\n",
    "A two-layer default MLP will be computed if ``estimator = None``. We also support user-defined MLP models. Any deeplift.layers functions can be used to measure the feature importance by setting parameter ``importance_func`` in its string form. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "981fe9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0\n",
      "DeepLift\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "MAKING A SESSION\n",
      "Computing scores for: NonlinearMxtsMode.RevealCancel\n",
      "598/598 [==============================] - 0s 422us/sample - loss: 0.4462\n",
      "Iter:  1\n",
      "DeepLift\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "Computing scores for: NonlinearMxtsMode.RevealCancel\n",
      "598/598 [==============================] - 0s 60us/sample - loss: 0.4490\n",
      "Iter:  2\n",
      "DeepLift\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "Computing scores for: NonlinearMxtsMode.RevealCancel\n",
      "598/598 [==============================] - 0s 62us/sample - loss: 0.3360\n",
      "Importance Function is  DeepLift_MLP\n",
      "          data model  Accuracy\n",
      "0  communities   MLP  0.446208\n",
      "1  communities   MLP  0.448972\n",
      "2  communities   MLP  0.336010\n",
      "           data        method criteria   K  Consistency  Accuracy\n",
      "0   communities  DeepLift_MLP      RBO   1        0.000      0.41\n",
      "1   communities  DeepLift_MLP      RBO   2        0.125      0.41\n",
      "2   communities  DeepLift_MLP      RBO   3        0.250      0.41\n",
      "3   communities  DeepLift_MLP      RBO   4        0.281      0.41\n",
      "4   communities  DeepLift_MLP      RBO   5        0.305      0.41\n",
      "5   communities  DeepLift_MLP      RBO   6        0.310      0.41\n",
      "6   communities  DeepLift_MLP      RBO   7        0.306      0.41\n",
      "7   communities  DeepLift_MLP      RBO   8        0.299      0.41\n",
      "8   communities  DeepLift_MLP      RBO   9        0.291      0.41\n",
      "9   communities  DeepLift_MLP      RBO  10        0.292      0.41\n",
      "10  communities  DeepLift_MLP      RBO  11        0.290      0.41\n",
      "11  communities  DeepLift_MLP      RBO  12        0.287      0.41\n",
      "12  communities  DeepLift_MLP      RBO  13        0.285      0.41\n",
      "13  communities  DeepLift_MLP      RBO  14        0.293      0.41\n",
      "14  communities  DeepLift_MLP      RBO  15        0.300      0.41\n",
      "15  communities  DeepLift_MLP      RBO  16        0.311      0.41\n",
      "16  communities  DeepLift_MLP      RBO  17        0.325      0.41\n",
      "17  communities  DeepLift_MLP      RBO  18        0.338      0.41\n",
      "18  communities  DeepLift_MLP      RBO  19        0.351      0.41\n",
      "19  communities  DeepLift_MLP      RBO  20        0.362      0.41\n",
      "20  communities  DeepLift_MLP      RBO  21        0.373      0.41\n",
      "21  communities  DeepLift_MLP      RBO  22        0.384      0.41\n",
      "22  communities  DeepLift_MLP      RBO  23        0.393      0.41\n",
      "23  communities  DeepLift_MLP      RBO  24        0.400      0.41\n",
      "24  communities  DeepLift_MLP      RBO  25        0.406      0.41\n",
      "25  communities  DeepLift_MLP      RBO  26        0.411      0.41\n",
      "26  communities  DeepLift_MLP      RBO  27        0.415      0.41\n",
      "27  communities  DeepLift_MLP      RBO  28        0.418      0.41\n",
      "28  communities  DeepLift_MLP      RBO  29        0.422      0.41\n",
      "29  communities  DeepLift_MLP      RBO  30        0.427      0.41\n",
      "30  communities  DeepLift_MLP  Jaccard   1        0.000      0.41\n",
      "31  communities  DeepLift_MLP  Jaccard   2        0.167      0.41\n",
      "32  communities  DeepLift_MLP  Jaccard   3        0.350      0.41\n",
      "33  communities  DeepLift_MLP  Jaccard   4        0.238      0.41\n",
      "34  communities  DeepLift_MLP  Jaccard   5        0.250      0.41\n",
      "35  communities  DeepLift_MLP  Jaccard   6        0.200      0.41\n",
      "36  communities  DeepLift_MLP  Jaccard   7        0.167      0.41\n",
      "37  communities  DeepLift_MLP  Jaccard   8        0.143      0.41\n",
      "38  communities  DeepLift_MLP  Jaccard   9        0.125      0.41\n",
      "39  communities  DeepLift_MLP  Jaccard  10        0.176      0.41\n",
      "40  communities  DeepLift_MLP  Jaccard  11        0.158      0.41\n",
      "41  communities  DeepLift_MLP  Jaccard  12        0.143      0.41\n",
      "42  communities  DeepLift_MLP  Jaccard  13        0.156      0.41\n",
      "43  communities  DeepLift_MLP  Jaccard  14        0.245      0.41\n",
      "44  communities  DeepLift_MLP  Jaccard  15        0.250      0.41\n",
      "45  communities  DeepLift_MLP  Jaccard  16        0.307      0.41\n",
      "46  communities  DeepLift_MLP  Jaccard  17        0.388      0.41\n",
      "47  communities  DeepLift_MLP  Jaccard  18        0.385      0.41\n",
      "48  communities  DeepLift_MLP  Jaccard  19        0.407      0.41\n",
      "49  communities  DeepLift_MLP  Jaccard  20        0.404      0.41\n",
      "50  communities  DeepLift_MLP  Jaccard  21        0.424      0.41\n",
      "51  communities  DeepLift_MLP  Jaccard  22        0.443      0.41\n",
      "52  communities  DeepLift_MLP  Jaccard  23        0.416      0.41\n",
      "53  communities  DeepLift_MLP  Jaccard  24        0.392      0.41\n",
      "54  communities  DeepLift_MLP  Jaccard  25        0.389      0.41\n",
      "55  communities  DeepLift_MLP  Jaccard  26        0.368      0.41\n",
      "56  communities  DeepLift_MLP  Jaccard  27        0.350      0.41\n",
      "57  communities  DeepLift_MLP  Jaccard  28        0.333      0.41\n",
      "58  communities  DeepLift_MLP  Jaccard  29        0.349      0.41\n",
      "59  communities  DeepLift_MLP  Jaccard  30        0.395      0.41\n",
      "          data model   Entropy    Purity\n",
      "0  communities   MLP  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from deeplift.layers import NonlinearMxtsMode\n",
    "import deeplift\n",
    "importance_func = 'NonlinearMxtsMode.RevealCancel'\n",
    "\n",
    "model_reg_mlp_dl=imlreliability.feature_importance.feature_impoReg_MLP(data_reg,\n",
    "                                                                           \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg_mlp_dl.fit()\n",
    "model_reg_mlp_dl.get_consistency(data_name='communities', estimator_name='MLP',impotance_func_name='DeepLift')\n",
    "print(model_reg_mlp_dl.accuracy)\n",
    "print(model_reg_mlp_dl.consistency)\n",
    "print(model_reg_mlp_dl.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1da12",
   "metadata": {},
   "source": [
    "#### 1.3.2. DeepExplain\n",
    "Here we construct a MLP model with two hidden layers as the prediction model, and epsilon-LRP as the post-hoc method to measure the feature importance, by setting ``importance_func='elrp'``. All other settings are the same as linear regression in 1.1. \n",
    "\n",
    "A two-layer default MLP will be computed if ``estimator = None``. We also support user-defined MLP models. Any DeepExplain function can be used to measure the feature importance by setting parameter ``importance_func`` in its string form. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99195e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0\n",
      "DeepExplain\n",
      "598/598 [==============================] - 0s 370us/sample - loss: 0.4554\n",
      "Iter:  1\n",
      "DeepExplain\n",
      "598/598 [==============================] - 0s 68us/sample - loss: 0.4359\n",
      "Iter:  2\n",
      "DeepExplain\n",
      "598/598 [==============================] - 0s 86us/sample - loss: 0.3315\n",
      "Importance Function is  elrp_MLP\n",
      "          data model  Accuracy\n",
      "0  communities   MLP  0.455388\n",
      "1  communities   MLP  0.435876\n",
      "2  communities   MLP  0.331540\n",
      "           data    method criteria   K  Consistency  Accuracy\n",
      "0   communities  elrp_MLP      RBO   1        1.000     0.408\n",
      "1   communities  elrp_MLP      RBO   2        0.750     0.408\n",
      "2   communities  elrp_MLP      RBO   3        0.611     0.408\n",
      "3   communities  elrp_MLP      RBO   4        0.521     0.408\n",
      "4   communities  elrp_MLP      RBO   5        0.537     0.408\n",
      "5   communities  elrp_MLP      RBO   6        0.558     0.408\n",
      "6   communities  elrp_MLP      RBO   7        0.581     0.408\n",
      "7   communities  elrp_MLP      RBO   8        0.602     0.408\n",
      "8   communities  elrp_MLP      RBO   9        0.628     0.408\n",
      "9   communities  elrp_MLP      RBO  10        0.640     0.408\n",
      "10  communities  elrp_MLP      RBO  11        0.648     0.408\n",
      "11  communities  elrp_MLP      RBO  12        0.653     0.408\n",
      "12  communities  elrp_MLP      RBO  13        0.662     0.408\n",
      "13  communities  elrp_MLP      RBO  14        0.676     0.408\n",
      "14  communities  elrp_MLP      RBO  15        0.686     0.408\n",
      "15  communities  elrp_MLP      RBO  16        0.698     0.408\n",
      "16  communities  elrp_MLP      RBO  17        0.709     0.408\n",
      "17  communities  elrp_MLP      RBO  18        0.717     0.408\n",
      "18  communities  elrp_MLP      RBO  19        0.727     0.408\n",
      "19  communities  elrp_MLP      RBO  20        0.737     0.408\n",
      "20  communities  elrp_MLP      RBO  21        0.745     0.408\n",
      "21  communities  elrp_MLP      RBO  22        0.750     0.408\n",
      "22  communities  elrp_MLP      RBO  23        0.756     0.408\n",
      "23  communities  elrp_MLP      RBO  24        0.764     0.408\n",
      "24  communities  elrp_MLP      RBO  25        0.772     0.408\n",
      "25  communities  elrp_MLP      RBO  26        0.780     0.408\n",
      "26  communities  elrp_MLP      RBO  27        0.786     0.408\n",
      "27  communities  elrp_MLP      RBO  28        0.791     0.408\n",
      "28  communities  elrp_MLP      RBO  29        0.795     0.408\n",
      "29  communities  elrp_MLP      RBO  30        0.799     0.408\n",
      "30  communities  elrp_MLP  Jaccard   1        1.000     0.408\n",
      "31  communities  elrp_MLP  Jaccard   2        0.333     0.408\n",
      "32  communities  elrp_MLP  Jaccard   3        0.200     0.408\n",
      "33  communities  elrp_MLP  Jaccard   4        0.143     0.408\n",
      "34  communities  elrp_MLP  Jaccard   5        0.429     0.408\n",
      "35  communities  elrp_MLP  Jaccard   6        0.500     0.408\n",
      "36  communities  elrp_MLP  Jaccard   7        0.556     0.408\n",
      "37  communities  elrp_MLP  Jaccard   8        0.600     0.408\n",
      "38  communities  elrp_MLP  Jaccard   9        0.718     0.408\n",
      "39  communities  elrp_MLP  Jaccard  10        0.603     0.408\n",
      "40  communities  elrp_MLP  Jaccard  11        0.579     0.408\n",
      "41  communities  elrp_MLP  Jaccard  12        0.563     0.408\n",
      "42  communities  elrp_MLP  Jaccard  13        0.631     0.408\n",
      "43  communities  elrp_MLP  Jaccard  14        0.757     0.408\n",
      "44  communities  elrp_MLP  Jaccard  15        0.716     0.408\n",
      "45  communities  elrp_MLP  Jaccard  16        0.778     0.408\n",
      "46  communities  elrp_MLP  Jaccard  17        0.789     0.408\n",
      "47  communities  elrp_MLP  Jaccard  18        0.757     0.408\n",
      "48  communities  elrp_MLP  Jaccard  19        0.810     0.408\n",
      "49  communities  elrp_MLP  Jaccard  20        0.861     0.408\n",
      "50  communities  elrp_MLP  Jaccard  21        0.826     0.408\n",
      "51  communities  elrp_MLP  Jaccard  22        0.760     0.408\n",
      "52  communities  elrp_MLP  Jaccard  23        0.805     0.408\n",
      "53  communities  elrp_MLP  Jaccard  24        0.883     0.408\n",
      "54  communities  elrp_MLP  Jaccard  25        0.923     0.408\n",
      "55  communities  elrp_MLP  Jaccard  26        0.963     0.408\n",
      "56  communities  elrp_MLP  Jaccard  27        0.895     0.408\n",
      "57  communities  elrp_MLP  Jaccard  28        0.867     0.408\n",
      "58  communities  elrp_MLP  Jaccard  29        0.842     0.408\n",
      "59  communities  elrp_MLP  Jaccard  30        0.820     0.408\n",
      "          data model   Entropy    Purity\n",
      "0  communities   MLP  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from deepexplain.tensorflow import DeepExplain\n",
    "importance_func ='elrp'\n",
    "\n",
    "## Two-layer default MLP will be computed if estimator =None. Can input user-defined MLP model\n",
    "\n",
    "model_reg_mlp_dl=imlreliability.feature_importance.feature_impoReg_MLP(data_reg,\n",
    "                                                                           \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_reg_mlp_dl.fit()\n",
    "model_reg_mlp_dl.get_consistency(data_name='communities', estimator_name='MLP',impotance_func_name='elrp')\n",
    "print(model_reg_mlp_dl.accuracy)\n",
    "print(model_reg_mlp_dl.consistency)\n",
    "print(model_reg_mlp_dl.prediction_consistency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-reducing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
