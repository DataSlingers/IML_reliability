{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3bbe3b",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66faeb56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import imlreliability\n",
    "import pandas as pd\n",
    "import numpy as np#### Load Packages dir(imlreliability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9c219",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-prague",
   "metadata": {},
   "source": [
    "Reliability test of feature importance techniques can be performed with the module imlreliability.feature_importance. Non-MLP techniques can be evaluated ``feature_impoReg`` for regression tasks and ``feature_impoClass`` for classification tasks. MLP-based techniques can be evaluated by ``feature_impoReg_MLP`` for regression tasks and ``feature_impoClass_MLP`` for classification tasks. \n",
    "\n",
    "Model agnostic techniques can be evaluated by specifying the parameter of importance function ``importance_func``. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-assist",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e7ef7",
   "metadata": {},
   "source": [
    "#### Load data\n",
    " We use the madelon classifiction data as an example for the following sections. The data has 2000 observations and 500 feature. We pre-process the data set by scaling and normalizing the predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beeaba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, normalize\n",
    "import tensorflow as tf\n",
    "x = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.data',header=None,sep=' ')\n",
    "y=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.labels',header=None,sep=' ')\n",
    "x=x.iloc[:,:500]\n",
    "x=np.array(x)\n",
    "y=y[0].tolist()\n",
    "\n",
    "### scale and normalize data \n",
    "x = normalize(scale(x))\n",
    "data_class=(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ce6ae",
   "metadata": {},
   "source": [
    "### 2.1. Model specific IML method\n",
    "\n",
    "The estimator is assumed to implement the scikit-learn estimator interface. To measure the feature importance, either estimator needs to provide a ``score`` function or ``scoring`` must be passed. For example, in logistic regression, the magnitude of coefficients is used to evaluate feature importance if there is no user-defined scoring function provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc58e",
   "metadata": {},
   "source": [
    "#### 2.1.1. Linear model\n",
    "\n",
    "Here we aim to evaluate the interpretation reliability of Logistic Ridge regression with cross validation, using the ``feature_impoReg``function. We use ``LogisticRegressionCV()`` from ``sklearn`` as our estimator. By setting ``importance_func=None``, the magnitude of coefficients will be used to evaluate feature importance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7527818e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "estimator=LogisticRegressionCV(cv=5,penalty='l2',solver='saga',max_iter=100)\n",
    "importance_func=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-divorce",
   "metadata": {},
   "source": [
    "We initialize the model with the ``mlreliability.feature_importance.feature_impoClass`` function. For illustration purpose, we run 3 repeats with 70%/30% train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "known-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use coefs as feature importance \n",
      "1\n",
      "Iter:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use coefs as feature importance \n",
      "2\n",
      "Iter:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use coefs as feature importance \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    }
   ],
   "source": [
    "model_class = imlreliability.feature_importance.feature_impoClass(data_class,estimator=estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-federation",
   "metadata": {},
   "source": [
    "The ``.get_consistency`` function results in three pandas dataframe: ``accuracy``: prediction accuracy on test set; ``consistency``: interpretation consistency measured by RBO, Jaccard score, or user-defined metrics; and prediction_consistency measured by prediction entropy and purity if ``get_prediction_consistency ==True``. \n",
    "\n",
    "The ``consistency`` pandas dataframe can be downloaded and upload to the dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sought-province",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Function is  Coef_LogisticRidge\n",
      "      data          model  Accuracy\n",
      "0  Madelon  LogisticRidge  0.571667\n",
      "1  Madelon  LogisticRidge  0.573333\n",
      "2  Madelon  LogisticRidge  0.583333\n",
      "       data              method criteria   K  Consistency  Accuracy\n",
      "0   Madelon  Coef_LogisticRidge      RBO   1        1.000     0.576\n",
      "1   Madelon  Coef_LogisticRidge      RBO   2        1.000     0.576\n",
      "2   Madelon  Coef_LogisticRidge      RBO   3        0.944     0.576\n",
      "3   Madelon  Coef_LogisticRidge      RBO   4        0.958     0.576\n",
      "4   Madelon  Coef_LogisticRidge      RBO   5        0.947     0.576\n",
      "5   Madelon  Coef_LogisticRidge      RBO   6        0.928     0.576\n",
      "6   Madelon  Coef_LogisticRidge      RBO   7        0.918     0.576\n",
      "7   Madelon  Coef_LogisticRidge      RBO   8        0.912     0.576\n",
      "8   Madelon  Coef_LogisticRidge      RBO   9        0.910     0.576\n",
      "9   Madelon  Coef_LogisticRidge      RBO  10        0.914     0.576\n",
      "10  Madelon  Coef_LogisticRidge      RBO  11        0.917     0.576\n",
      "11  Madelon  Coef_LogisticRidge      RBO  12        0.917     0.576\n",
      "12  Madelon  Coef_LogisticRidge      RBO  13        0.921     0.576\n",
      "13  Madelon  Coef_LogisticRidge      RBO  14        0.921     0.576\n",
      "14  Madelon  Coef_LogisticRidge      RBO  15        0.918     0.576\n",
      "15  Madelon  Coef_LogisticRidge      RBO  16        0.911     0.576\n",
      "16  Madelon  Coef_LogisticRidge      RBO  17        0.903     0.576\n",
      "17  Madelon  Coef_LogisticRidge      RBO  18        0.893     0.576\n",
      "18  Madelon  Coef_LogisticRidge      RBO  19        0.882     0.576\n",
      "19  Madelon  Coef_LogisticRidge      RBO  20        0.871     0.576\n",
      "20  Madelon  Coef_LogisticRidge      RBO  21        0.860     0.576\n",
      "21  Madelon  Coef_LogisticRidge      RBO  22        0.849     0.576\n",
      "22  Madelon  Coef_LogisticRidge      RBO  23        0.838     0.576\n",
      "23  Madelon  Coef_LogisticRidge      RBO  24        0.827     0.576\n",
      "24  Madelon  Coef_LogisticRidge      RBO  25        0.816     0.576\n",
      "25  Madelon  Coef_LogisticRidge      RBO  26        0.806     0.576\n",
      "26  Madelon  Coef_LogisticRidge      RBO  27        0.797     0.576\n",
      "27  Madelon  Coef_LogisticRidge      RBO  28        0.789     0.576\n",
      "28  Madelon  Coef_LogisticRidge      RBO  29        0.782     0.576\n",
      "29  Madelon  Coef_LogisticRidge      RBO  30        0.775     0.576\n",
      "30  Madelon  Coef_LogisticRidge  Jaccard   1        1.000     0.576\n",
      "31  Madelon  Coef_LogisticRidge  Jaccard   2        1.000     0.576\n",
      "32  Madelon  Coef_LogisticRidge  Jaccard   3        0.750     0.576\n",
      "33  Madelon  Coef_LogisticRidge  Jaccard   4        1.000     0.576\n",
      "34  Madelon  Coef_LogisticRidge  Jaccard   5        0.833     0.576\n",
      "35  Madelon  Coef_LogisticRidge  Jaccard   6        0.750     0.576\n",
      "36  Madelon  Coef_LogisticRidge  Jaccard   7        0.778     0.576\n",
      "37  Madelon  Coef_LogisticRidge  Jaccard   8        0.800     0.576\n",
      "38  Madelon  Coef_LogisticRidge  Jaccard   9        0.818     0.576\n",
      "39  Madelon  Coef_LogisticRidge  Jaccard  10        0.909     0.576\n",
      "40  Madelon  Coef_LogisticRidge  Jaccard  11        0.917     0.576\n",
      "41  Madelon  Coef_LogisticRidge  Jaccard  12        0.846     0.576\n",
      "42  Madelon  Coef_LogisticRidge  Jaccard  13        0.929     0.576\n",
      "43  Madelon  Coef_LogisticRidge  Jaccard  14        0.867     0.576\n",
      "44  Madelon  Coef_LogisticRidge  Jaccard  15        0.765     0.576\n",
      "45  Madelon  Coef_LogisticRidge  Jaccard  16        0.684     0.576\n",
      "46  Madelon  Coef_LogisticRidge  Jaccard  17        0.619     0.576\n",
      "47  Madelon  Coef_LogisticRidge  Jaccard  18        0.565     0.576\n",
      "48  Madelon  Coef_LogisticRidge  Jaccard  19        0.520     0.576\n",
      "49  Madelon  Coef_LogisticRidge  Jaccard  20        0.510     0.576\n",
      "50  Madelon  Coef_LogisticRidge  Jaccard  21        0.474     0.576\n",
      "51  Madelon  Coef_LogisticRidge  Jaccard  22        0.443     0.576\n",
      "52  Madelon  Coef_LogisticRidge  Jaccard  23        0.416     0.576\n",
      "53  Madelon  Coef_LogisticRidge  Jaccard  24        0.413     0.576\n",
      "54  Madelon  Coef_LogisticRidge  Jaccard  25        0.390     0.576\n",
      "55  Madelon  Coef_LogisticRidge  Jaccard  26        0.389     0.576\n",
      "56  Madelon  Coef_LogisticRidge  Jaccard  27        0.388     0.576\n",
      "57  Madelon  Coef_LogisticRidge  Jaccard  28        0.401     0.576\n",
      "58  Madelon  Coef_LogisticRidge  Jaccard  29        0.398     0.576\n",
      "59  Madelon  Coef_LogisticRidge  Jaccard  30        0.412     0.576\n",
      "      data          model   Entropy    Purity\n",
      "0  Madelon  LogisticRidge  0.021204  0.969409\n"
     ]
    }
   ],
   "source": [
    "model_class.get_consistency(data_name='Madelon', estimator_name='LogisticRidge',impotance_func_name='Coef')\n",
    "print(model_class.accuracy)\n",
    "print(model_class.consistency)\n",
    "print(model_class.prediction_consistency)\n",
    "\n",
    "## model_class.consistency.to_csv('consis_test_fi_class.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cbfa8",
   "metadata": {},
   "source": [
    "#### 2.1.2. Tree-base model\n",
    "Here we aim to evaluate the interpretation reliability of random forest, using the ``feature_impoClass``function.. We use ``RandomForestClassifier()`` from ``sklearn`` as our estimator. By setting ``importance_func=None``, the default feature importance ``feature_importances_`` of the ``RandomForestClassifier()`` function will be used to evaluate feature importance. \n",
    "All other settings are the same as logistic regression in 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628a2df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n",
      "use feature_importances_ as feature importance \n",
      "1\n",
      "Iter:  1\n",
      "use feature_importances_ as feature importance \n",
      "2\n",
      "Iter:  2\n",
      "use feature_importances_ as feature importance \n",
      "Importance Function is  FI_RF\n",
      "      data model  Accuracy\n",
      "0  Madelon    RF  0.668333\n",
      "1  Madelon    RF  0.633333\n",
      "2  Madelon    RF  0.691667\n",
      "       data method criteria   K  Consistency  Accuracy\n",
      "0   Madelon  FI_RF      RBO   1        0.000     0.664\n",
      "1   Madelon  FI_RF      RBO   2        0.375     0.664\n",
      "2   Madelon  FI_RF      RBO   3        0.528     0.664\n",
      "3   Madelon  FI_RF      RBO   4        0.583     0.664\n",
      "4   Madelon  FI_RF      RBO   5        0.627     0.664\n",
      "5   Madelon  FI_RF      RBO   6        0.633     0.664\n",
      "6   Madelon  FI_RF      RBO   7        0.645     0.664\n",
      "7   Madelon  FI_RF      RBO   8        0.650     0.664\n",
      "8   Madelon  FI_RF      RBO   9        0.664     0.664\n",
      "9   Madelon  FI_RF      RBO  10        0.683     0.664\n",
      "10  Madelon  FI_RF      RBO  11        0.695     0.664\n",
      "11  Madelon  FI_RF      RBO  12        0.710     0.664\n",
      "12  Madelon  FI_RF      RBO  13        0.718     0.664\n",
      "13  Madelon  FI_RF      RBO  14        0.723     0.664\n",
      "14  Madelon  FI_RF      RBO  15        0.728     0.664\n",
      "15  Madelon  FI_RF      RBO  16        0.737     0.664\n",
      "16  Madelon  FI_RF      RBO  17        0.747     0.664\n",
      "17  Madelon  FI_RF      RBO  18        0.757     0.664\n",
      "18  Madelon  FI_RF      RBO  19        0.767     0.664\n",
      "19  Madelon  FI_RF      RBO  20        0.775     0.664\n",
      "20  Madelon  FI_RF      RBO  21        0.780     0.664\n",
      "21  Madelon  FI_RF      RBO  22        0.783     0.664\n",
      "22  Madelon  FI_RF      RBO  23        0.785     0.664\n",
      "23  Madelon  FI_RF      RBO  24        0.786     0.664\n",
      "24  Madelon  FI_RF      RBO  25        0.785     0.664\n",
      "25  Madelon  FI_RF      RBO  26        0.783     0.664\n",
      "26  Madelon  FI_RF      RBO  27        0.780     0.664\n",
      "27  Madelon  FI_RF      RBO  28        0.777     0.664\n",
      "28  Madelon  FI_RF      RBO  29        0.773     0.664\n",
      "29  Madelon  FI_RF      RBO  30        0.769     0.664\n",
      "30  Madelon  FI_RF  Jaccard   1        0.000     0.664\n",
      "31  Madelon  FI_RF  Jaccard   2        0.667     0.664\n",
      "32  Madelon  FI_RF  Jaccard   3        0.750     0.664\n",
      "33  Madelon  FI_RF  Jaccard   4        0.600     0.664\n",
      "34  Madelon  FI_RF  Jaccard   5        0.667     0.664\n",
      "35  Madelon  FI_RF  Jaccard   6        0.500     0.664\n",
      "36  Madelon  FI_RF  Jaccard   7        0.556     0.664\n",
      "37  Madelon  FI_RF  Jaccard   8        0.527     0.664\n",
      "38  Madelon  FI_RF  Jaccard   9        0.636     0.664\n",
      "39  Madelon  FI_RF  Jaccard  10        0.742     0.664\n",
      "40  Madelon  FI_RF  Jaccard  11        0.692     0.664\n",
      "41  Madelon  FI_RF  Jaccard  12        0.780     0.664\n",
      "42  Madelon  FI_RF  Jaccard  13        0.679     0.664\n",
      "43  Madelon  FI_RF  Jaccard  14        0.647     0.664\n",
      "44  Madelon  FI_RF  Jaccard  15        0.667     0.664\n",
      "45  Madelon  FI_RF  Jaccard  16        0.778     0.664\n",
      "46  Madelon  FI_RF  Jaccard  17        0.839     0.664\n",
      "47  Madelon  FI_RF  Jaccard  18        0.847     0.664\n",
      "48  Madelon  FI_RF  Jaccard  19        0.900     0.664\n",
      "49  Madelon  FI_RF  Jaccard  20        0.861     0.664\n",
      "50  Madelon  FI_RF  Jaccard  21        0.788     0.664\n",
      "51  Madelon  FI_RF  Jaccard  22        0.760     0.664\n",
      "52  Madelon  FI_RF  Jaccard  23        0.704     0.664\n",
      "53  Madelon  FI_RF  Jaccard  24        0.655     0.664\n",
      "54  Madelon  FI_RF  Jaccard  25        0.613     0.664\n",
      "55  Madelon  FI_RF  Jaccard  26        0.576     0.664\n",
      "56  Madelon  FI_RF  Jaccard  27        0.543     0.664\n",
      "57  Madelon  FI_RF  Jaccard  28        0.535     0.664\n",
      "58  Madelon  FI_RF  Jaccard  29        0.507     0.664\n",
      "59  Madelon  FI_RF  Jaccard  30        0.500     0.664\n",
      "      data model   Entropy    Purity\n",
      "0  Madelon    RF  0.040499  0.941572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator=RandomForestClassifier()\n",
    "importance_func=None\n",
    "\n",
    "\n",
    "model_class_tree=imlreliability.feature_importance.feature_impoClass(data_class,estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class_tree.fit()\n",
    "model_class_tree.get_consistency(data_name='Madelon', estimator_name='RF',impotance_func_name='FI')\n",
    "print(model_class_tree.accuracy)\n",
    "print(model_class_tree.consistency)\n",
    "print(model_class_tree.prediction_consistency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8dcc1e",
   "metadata": {},
   "source": [
    "### 2.2. Model agnostic \n",
    "For model agnostic methods to measure feature importance, we provide built-in importance functions from package shap and perumutation function from sklearn.inspection. The imlreliability also support elf-defined importance function, with three argument: ``(fitted model, training x, training y)``, and 1 output importance score in forms of list or array:\n",
    "\n",
    "``importance_func(self.fitted,x_train, y_train)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bad43b",
   "metadata": {},
   "source": [
    "#### 2.2.1. Permutation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-specific",
   "metadata": {},
   "source": [
    "##### 2.2.1.1. Random Forest + Permutation\n",
    "Here we use random forest to consturct the prediction model using the ``feature_impoClass``function, and permutation as the post-hoc method to measure the feature importance, by setting ``importance_func=permutation_importance``. All other settings are the same as in 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06725fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iter:  0\n",
      "1\n",
      "Iter:  1\n",
      "2\n",
      "Iter:  2\n",
      "Importance Function is  Permutation_RF\n",
      "      data model  Accuracy\n",
      "0  Madelon    RF  0.643333\n",
      "1  Madelon    RF  0.641667\n",
      "2  Madelon    RF  0.661667\n",
      "       data          method criteria   K  Consistency  Accuracy\n",
      "0   Madelon  Permutation_RF      RBO   1          0.0     0.649\n",
      "1   Madelon  Permutation_RF      RBO   2          0.0     0.649\n",
      "2   Madelon  Permutation_RF      RBO   3          0.0     0.649\n",
      "3   Madelon  Permutation_RF      RBO   4          0.0     0.649\n",
      "4   Madelon  Permutation_RF      RBO   5          0.0     0.649\n",
      "5   Madelon  Permutation_RF      RBO   6          0.0     0.649\n",
      "6   Madelon  Permutation_RF      RBO   7          0.0     0.649\n",
      "7   Madelon  Permutation_RF      RBO   8          0.0     0.649\n",
      "8   Madelon  Permutation_RF      RBO   9          0.0     0.649\n",
      "9   Madelon  Permutation_RF      RBO  10          0.0     0.649\n",
      "10  Madelon  Permutation_RF      RBO  11          0.0     0.649\n",
      "11  Madelon  Permutation_RF      RBO  12          0.0     0.649\n",
      "12  Madelon  Permutation_RF      RBO  13          0.0     0.649\n",
      "13  Madelon  Permutation_RF      RBO  14          0.0     0.649\n",
      "14  Madelon  Permutation_RF      RBO  15          0.0     0.649\n",
      "15  Madelon  Permutation_RF      RBO  16          0.0     0.649\n",
      "16  Madelon  Permutation_RF      RBO  17          0.0     0.649\n",
      "17  Madelon  Permutation_RF      RBO  18          0.0     0.649\n",
      "18  Madelon  Permutation_RF      RBO  19          0.0     0.649\n",
      "19  Madelon  Permutation_RF      RBO  20          0.0     0.649\n",
      "20  Madelon  Permutation_RF      RBO  21          0.0     0.649\n",
      "21  Madelon  Permutation_RF      RBO  22          0.0     0.649\n",
      "22  Madelon  Permutation_RF      RBO  23          0.0     0.649\n",
      "23  Madelon  Permutation_RF      RBO  24          0.0     0.649\n",
      "24  Madelon  Permutation_RF      RBO  25          0.0     0.649\n",
      "25  Madelon  Permutation_RF      RBO  26          0.0     0.649\n",
      "26  Madelon  Permutation_RF      RBO  27          0.0     0.649\n",
      "27  Madelon  Permutation_RF      RBO  28          0.0     0.649\n",
      "28  Madelon  Permutation_RF      RBO  29          0.0     0.649\n",
      "29  Madelon  Permutation_RF      RBO  30          0.0     0.649\n",
      "30  Madelon  Permutation_RF  Jaccard   1          0.0     0.649\n",
      "31  Madelon  Permutation_RF  Jaccard   2          0.0     0.649\n",
      "32  Madelon  Permutation_RF  Jaccard   3          0.0     0.649\n",
      "33  Madelon  Permutation_RF  Jaccard   4          0.0     0.649\n",
      "34  Madelon  Permutation_RF  Jaccard   5          0.0     0.649\n",
      "35  Madelon  Permutation_RF  Jaccard   6          0.0     0.649\n",
      "36  Madelon  Permutation_RF  Jaccard   7          0.0     0.649\n",
      "37  Madelon  Permutation_RF  Jaccard   8          0.0     0.649\n",
      "38  Madelon  Permutation_RF  Jaccard   9          0.0     0.649\n",
      "39  Madelon  Permutation_RF  Jaccard  10          0.0     0.649\n",
      "40  Madelon  Permutation_RF  Jaccard  11          0.0     0.649\n",
      "41  Madelon  Permutation_RF  Jaccard  12          0.0     0.649\n",
      "42  Madelon  Permutation_RF  Jaccard  13          0.0     0.649\n",
      "43  Madelon  Permutation_RF  Jaccard  14          0.0     0.649\n",
      "44  Madelon  Permutation_RF  Jaccard  15          0.0     0.649\n",
      "45  Madelon  Permutation_RF  Jaccard  16          0.0     0.649\n",
      "46  Madelon  Permutation_RF  Jaccard  17          0.0     0.649\n",
      "47  Madelon  Permutation_RF  Jaccard  18          0.0     0.649\n",
      "48  Madelon  Permutation_RF  Jaccard  19          0.0     0.649\n",
      "49  Madelon  Permutation_RF  Jaccard  20          0.0     0.649\n",
      "50  Madelon  Permutation_RF  Jaccard  21          0.0     0.649\n",
      "51  Madelon  Permutation_RF  Jaccard  22          0.0     0.649\n",
      "52  Madelon  Permutation_RF  Jaccard  23          0.0     0.649\n",
      "53  Madelon  Permutation_RF  Jaccard  24          0.0     0.649\n",
      "54  Madelon  Permutation_RF  Jaccard  25          0.0     0.649\n",
      "55  Madelon  Permutation_RF  Jaccard  26          0.0     0.649\n",
      "56  Madelon  Permutation_RF  Jaccard  27          0.0     0.649\n",
      "57  Madelon  Permutation_RF  Jaccard  28          0.0     0.649\n",
      "58  Madelon  Permutation_RF  Jaccard  29          0.0     0.649\n",
      "59  Madelon  Permutation_RF  Jaccard  30          0.0     0.649\n",
      "      data model   Entropy    Purity\n",
      "0  Madelon    RF  0.035534  0.948735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "estimator=RandomForestClassifier()\n",
    "importance_func = permutation_importance ## change the importance function to be permutation \n",
    "\n",
    "\n",
    "model_class_tree_per=imlreliability.feature_importance.feature_impoClass(data_class,estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class_tree_per.fit()\n",
    "model_class_tree_per.get_consistency(data_name='Madelon', estimator_name='RF',impotance_func_name='Permutation')\n",
    "print(model_class_tree_per.accuracy)\n",
    "print(model_class_tree_per.consistency)\n",
    "print(model_class_tree_per.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-probability",
   "metadata": {},
   "source": [
    "##### 2.2.1.2. MLP + Permutation\n",
    "Here we construct a MLP model with two hidden layers as the prediction model, and permutation as the post-hoc method to measure the feature importance, by setting ``importance_func=permutation_importance``. Note that here we use the ``feature_impoClass_MLP`` function for MLP-based techniques. All other settings are the same as logistic regression in 2.1.1. \n",
    "\n",
    "A two-layer default MLP will be computed if ``estimator = None``. We also support user-defined MLP models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Iter:  0\n",
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "['eli5', 'sklearn', 'permutation_importance']\n",
      "600/600 [==============================] - 0s 126us/sample - loss: 0.6809 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6809 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 54us/sample - loss: 0.6808 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 48us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 46us/sample - loss: 0.6810 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 56us/sample - loss: 0.6810 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 49us/sample - loss: 0.6811 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 48us/sample - loss: 0.6810 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 50us/sample - loss: 0.6811 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 58us/sample - loss: 0.6804 - acc: 0.5600\n",
      "600/600 [==============================] - 0s 63us/sample - loss: 0.6810 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 76us/sample - loss: 0.6812 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 76us/sample - loss: 0.6810 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 61us/sample - loss: 0.6807 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 56us/sample - loss: 0.6807 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 60us/sample - loss: 0.6808 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 52us/sample - loss: 0.6809 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 53us/sample - loss: 0.6813 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 55us/sample - loss: 0.6807 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 54us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 60us/sample - loss: 0.6810 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 65us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 63us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 60us/sample - loss: 0.6809 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 58us/sample - loss: 0.6808 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 63us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 62us/sample - loss: 0.6806 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 64us/sample - loss: 0.6809 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 47us/sample - loss: 0.6811 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 46us/sample - loss: 0.6807 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 49us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 51us/sample - loss: 0.6805 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 64us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 48us/sample - loss: 0.6809 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 47us/sample - loss: 0.6809 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 76us/sample - loss: 0.6808 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 47us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 49us/sample - loss: 0.6810 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 52us/sample - loss: 0.6806 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 55us/sample - loss: 0.6811 - acc: 0.5583\n",
      "600/600 [==============================] - 0s 56us/sample - loss: 0.6808 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 56us/sample - loss: 0.6801 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 56us/sample - loss: 0.6813 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 120us/sample - loss: 0.6808 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 45us/sample - loss: 0.6814 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6809 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6811 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6810 - acc: 0.5450\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6808 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6818 - acc: 0.5583\n",
      "600/600 [==============================] - 0s 41us/sample - loss: 0.6814 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6810 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6812 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6812 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 41us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 42us/sample - loss: 0.6813 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 42us/sample - loss: 0.6810 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 49us/sample - loss: 0.6807 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 45us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6807 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 46us/sample - loss: 0.6810 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 42us/sample - loss: 0.6807 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 53us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6829 - acc: 0.5450\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 46us/sample - loss: 0.6813 - acc: 0.5600\n",
      "600/600 [==============================] - 0s 41us/sample - loss: 0.6810 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6807 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 51us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 45us/sample - loss: 0.6809 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 41us/sample - loss: 0.6809 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6810 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6811 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6802 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 50us/sample - loss: 0.6809 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 45us/sample - loss: 0.6808 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 49us/sample - loss: 0.6811 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 51us/sample - loss: 0.6810 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 55us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 52us/sample - loss: 0.6812 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 58us/sample - loss: 0.6806 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 50us/sample - loss: 0.6809 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 42us/sample - loss: 0.6806 - acc: 0.5600\n",
      "600/600 [==============================] - 0s 45us/sample - loss: 0.6809 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 46us/sample - loss: 0.6810 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6804 - acc: 0.5600\n",
      "600/600 [==============================] - 0s 43us/sample - loss: 0.6810 - acc: 0.5550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 49us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 66us/sample - loss: 0.6806 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 44us/sample - loss: 0.6812 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 39us/sample - loss: 0.6803 - acc: 0.5600\n",
      "600/600 [==============================] - 0s 42us/sample - loss: 0.6807 - acc: 0.5467\n",
      "600/600 [==============================] - 0s 38us/sample - loss: 0.6808 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6805 - acc: 0.5567\n",
      "600/600 [==============================] - 0s 38us/sample - loss: 0.6810 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 39us/sample - loss: 0.6811 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 38us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 39us/sample - loss: 0.6809 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6809 - acc: 0.5533\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6808 - acc: 0.5517\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6806 - acc: 0.5500\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6811 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 42us/sample - loss: 0.6804 - acc: 0.5550\n",
      "600/600 [==============================] - 0s 41us/sample - loss: 0.6821 - acc: 0.5667\n",
      "600/600 [==============================] - 0s 40us/sample - loss: 0.6810 - acc: 0.5483\n",
      "600/600 [==============================] - 0s 58us/sample - loss: 0.6808 - acc: 0.5550\n"
     ]
    }
   ],
   "source": [
    "from eli5.sklearn import PermutationImportance\n",
    "importance_func =PermutationImportance\n",
    "\n",
    "model_class_mlp_dl=imlreliability.feature_importance.feature_impoClass_MLP(data_class,\n",
    "                                                                           \n",
    "                 importance_func=PermutationImportance,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class_mlp_dl.fit()\n",
    "model_class_mlp_dl.get_consistency(data_name='Madelon', estimator_name='MLP',impotance_func_name='Permutation')\n",
    "print(model_class_mlp_dl.accuracy)\n",
    "print(model_class_mlp_dl.consistency)\n",
    "print(model_class_mlp_dl.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce05eb",
   "metadata": {},
   "source": [
    "#### 2.2.2. Shapley Value \n",
    "\n",
    "Here we use random forest to consturct the prediction model, and SHAP as the post-hoc method to measure the feature importance, by setting ``importance_func=shap.TreeExplainer``. All other settings are the same as logistic regression in 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-ferry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "estimator=RandomForestClassifier()\n",
    "importance_func = shap.TreeExplainer ## change the importance function to be SHAP \n",
    "\n",
    "\n",
    "model_class_tree_shap=imlreliability.feature_importance.feature_impoClass(data_class,estimator, \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class_tree_shap.fit()\n",
    "model_class_tree_shap.get_consistency(data_name='Madelon', estimator_name='RF',impotance_func_name='SHAP')\n",
    "print(model_class_tree_shap.accuracy)\n",
    "print(model_class_tree_shap.consistency)\n",
    "print(model_class_tree_shap.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523664c",
   "metadata": {},
   "source": [
    "### 2.3. MLP specific models \n",
    "We have built-in functions to run functions from ``deepexplain`` and  ``deeplift`` packages. As well permutation and shapley values. User can either input a function or a string from [\n",
    "                        'zero',\n",
    "                        'saliency',\n",
    "                        'grad*input',\n",
    "                        'intgrad',\n",
    "                        'elrp',\n",
    "                        'deeplift',\n",
    "                        'occlusion',\n",
    "                        'shapley_sampling'] to run deepExplain. \n",
    "Or input strings from ['NonlinearMxtsMode.RevealCancel','NonlinearMxtsMode.GuidedBackprop'...] to run corresponding functions in deeplift. \n",
    "\n",
    "\n",
    "imlreliability package also support self-defined importance function, with three argument: ``(fitted model, training x, training y)``, and 1 output importance score in forms of list or array:``importance_func(model,x_train, y_train)``. \n",
    "\n",
    "And the defined estimator needs to be form of :\n",
    "      \n",
    "```Python\n",
    "def _base_model_classification():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(M, input_dim=M, activation='relu'))\n",
    "    model.add(Dense(M, input_dim=M, activation='relu'))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "       \n",
    "    return model  \n",
    "```\n",
    "\n",
    "And the trained MLP model is saved as .h5 file. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a78df",
   "metadata": {},
   "source": [
    "#### 2.3.1. Deeplift\n",
    "\n",
    "Here we construct a MLP model with two hidden layers as the prediction model, and deeplift as the post-hoc method to measure the feature importance, by setting ``importance_func='NonlinearMxtsMode.RevealCancel'``. All other settings are the same as linear regression in 1.1. \n",
    "\n",
    "A two-layer default MLP will be computed if ``estimator = None``. We also support user-defined MLP models. Any deeplift.layers functions can be used to measure the feature importance by setting parameter ``importance_func`` in its string form. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981fe9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0\n",
      "DeepLift\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "Computing scores for: NonlinearMxtsMode.RevealCancel\n",
      "598/598 [==============================] - 0s 232us/sample - loss: 0.4724\n",
      "Iter:  1\n",
      "DeepLift\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "Computing scores for: NonlinearMxtsMode.RevealCancel\n",
      "598/598 [==============================] - 0s 66us/sample - loss: 0.4359\n",
      "Iter:  2\n",
      "DeepLift\n",
      "nonlinear_mxts_mode is set to: RevealCancel\n",
      "Computing scores for: NonlinearMxtsMode.RevealCancel\n",
      "598/598 [==============================] - 0s 58us/sample - loss: 0.3315\n",
      "Importance Function is  DeepLift_MLP\n",
      "          data model  Accuracy\n",
      "0  communities   MLP  0.472432\n",
      "1  communities   MLP  0.435864\n",
      "2  communities   MLP  0.331513\n",
      "           data        method criteria   K  Consistency  Accuracy\n",
      "0   communities  DeepLift_MLP      RBO   1        0.000     0.413\n",
      "1   communities  DeepLift_MLP      RBO   2        0.000     0.413\n",
      "2   communities  DeepLift_MLP      RBO   3        0.000     0.413\n",
      "3   communities  DeepLift_MLP      RBO   4        0.000     0.413\n",
      "4   communities  DeepLift_MLP      RBO   5        0.000     0.413\n",
      "5   communities  DeepLift_MLP      RBO   6        0.014     0.413\n",
      "6   communities  DeepLift_MLP      RBO   7        0.032     0.413\n",
      "7   communities  DeepLift_MLP      RBO   8        0.044     0.413\n",
      "8   communities  DeepLift_MLP      RBO   9        0.051     0.413\n",
      "9   communities  DeepLift_MLP      RBO  10        0.061     0.413\n",
      "10  communities  DeepLift_MLP      RBO  11        0.080     0.413\n",
      "11  communities  DeepLift_MLP      RBO  12        0.095     0.413\n",
      "12  communities  DeepLift_MLP      RBO  13        0.105     0.413\n",
      "13  communities  DeepLift_MLP      RBO  14        0.115     0.413\n",
      "14  communities  DeepLift_MLP      RBO  15        0.125     0.413\n",
      "15  communities  DeepLift_MLP      RBO  16        0.137     0.413\n",
      "16  communities  DeepLift_MLP      RBO  17        0.150     0.413\n",
      "17  communities  DeepLift_MLP      RBO  18        0.160     0.413\n",
      "18  communities  DeepLift_MLP      RBO  19        0.170     0.413\n",
      "19  communities  DeepLift_MLP      RBO  20        0.177     0.413\n",
      "20  communities  DeepLift_MLP      RBO  21        0.185     0.413\n",
      "21  communities  DeepLift_MLP      RBO  22        0.192     0.413\n",
      "22  communities  DeepLift_MLP      RBO  23        0.199     0.413\n",
      "23  communities  DeepLift_MLP      RBO  24        0.206     0.413\n",
      "24  communities  DeepLift_MLP      RBO  25        0.212     0.413\n",
      "25  communities  DeepLift_MLP      RBO  26        0.219     0.413\n",
      "26  communities  DeepLift_MLP      RBO  27        0.225     0.413\n",
      "27  communities  DeepLift_MLP      RBO  28        0.231     0.413\n",
      "28  communities  DeepLift_MLP      RBO  29        0.237     0.413\n",
      "29  communities  DeepLift_MLP      RBO  30        0.242     0.413\n",
      "30  communities  DeepLift_MLP  Jaccard   1        0.000     0.413\n",
      "31  communities  DeepLift_MLP  Jaccard   2        0.000     0.413\n",
      "32  communities  DeepLift_MLP  Jaccard   3        0.000     0.413\n",
      "33  communities  DeepLift_MLP  Jaccard   4        0.000     0.413\n",
      "34  communities  DeepLift_MLP  Jaccard   5        0.000     0.413\n",
      "35  communities  DeepLift_MLP  Jaccard   6        0.045     0.413\n",
      "36  communities  DeepLift_MLP  Jaccard   7        0.077     0.413\n",
      "37  communities  DeepLift_MLP  Jaccard   8        0.067     0.413\n",
      "38  communities  DeepLift_MLP  Jaccard   9        0.059     0.413\n",
      "39  communities  DeepLift_MLP  Jaccard  10        0.082     0.413\n",
      "40  communities  DeepLift_MLP  Jaccard  11        0.158     0.413\n",
      "41  communities  DeepLift_MLP  Jaccard  12        0.143     0.413\n",
      "42  communities  DeepLift_MLP  Jaccard  13        0.130     0.413\n",
      "43  communities  DeepLift_MLP  Jaccard  14        0.143     0.413\n",
      "44  communities  DeepLift_MLP  Jaccard  15        0.156     0.413\n",
      "45  communities  DeepLift_MLP  Jaccard  16        0.187     0.413\n",
      "46  communities  DeepLift_MLP  Jaccard  17        0.214     0.413\n",
      "47  communities  DeepLift_MLP  Jaccard  18        0.200     0.413\n",
      "48  communities  DeepLift_MLP  Jaccard  19        0.207     0.413\n",
      "49  communities  DeepLift_MLP  Jaccard  20        0.194     0.413\n",
      "50  communities  DeepLift_MLP  Jaccard  21        0.200     0.413\n",
      "51  communities  DeepLift_MLP  Jaccard  22        0.206     0.413\n",
      "52  communities  DeepLift_MLP  Jaccard  23        0.211     0.413\n",
      "53  communities  DeepLift_MLP  Jaccard  24        0.231     0.413\n",
      "54  communities  DeepLift_MLP  Jaccard  25        0.220     0.413\n",
      "55  communities  DeepLift_MLP  Jaccard  26        0.238     0.413\n",
      "56  communities  DeepLift_MLP  Jaccard  27        0.242     0.413\n",
      "57  communities  DeepLift_MLP  Jaccard  28        0.244     0.413\n",
      "58  communities  DeepLift_MLP  Jaccard  29        0.247     0.413\n",
      "59  communities  DeepLift_MLP  Jaccard  30        0.250     0.413\n",
      "          data model   Entropy    Purity\n",
      "0  communities   MLP  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from deeplift.layers import NonlinearMxtsMode\n",
    "import deeplift\n",
    "importance_func = 'NonlinearMxtsMode.RevealCancel'\n",
    "\n",
    "## Two-layer default MLP will be computed if estimator =None. Can input user-defined MLP model\n",
    "\n",
    "model_class_mlp_dl=imlreliability.feature_importance.feature_impoClass_MLP(data_class,\n",
    "                                                                           \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class_mlp_dl.fit()\n",
    "model_class_mlp_dl.get_consistency(data_name='Madelon', estimator_name='MLP',impotance_func_name='DeepLift')\n",
    "print(model_class_mlp_dl.accuracy)\n",
    "print(model_class_mlp_dl.consistency)\n",
    "print(model_class_mlp_dl.prediction_consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1da12",
   "metadata": {},
   "source": [
    "#### 2.3.2. DeepExplain\n",
    "Here we construct a MLP model with two hidden layers as the prediction model, and epsilon-LRP as the post-hoc method to measure the feature importance, by setting ``importance_func='elrp'``. All other settings are the same as logistic regression in 2.1. \n",
    "\n",
    "A two-layer default MLP will be computed if ``estimator = None``. We also support user-defined MLP models. Any DeepExplain function can be used to measure the feature importance by setting parameter ``importance_func`` in its string form. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99195e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Iter:  0\n",
      "WARNING:tensorflow:From /Users/alice/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "DeepExplain\n",
      "598/598 [==============================] - 0s 119us/sample - loss: 0.4488\n",
      "Iter:  1\n",
      "DeepExplain\n",
      "598/598 [==============================] - 0s 51us/sample - loss: 0.4519\n",
      "Iter:  2\n",
      "DeepExplain\n",
      "598/598 [==============================] - 0s 59us/sample - loss: 0.3333\n",
      "Importance Function is  elrp_MLP\n",
      "          data model  Accuracy\n",
      "0  communities   MLP  0.448769\n",
      "1  communities   MLP  0.451854\n",
      "2  communities   MLP  0.333314\n",
      "           data    method criteria   K  Consistency  Accuracy\n",
      "0   communities  elrp_MLP      RBO   1        1.000     0.411\n",
      "1   communities  elrp_MLP      RBO   2        0.750     0.411\n",
      "2   communities  elrp_MLP      RBO   3        0.722     0.411\n",
      "3   communities  elrp_MLP      RBO   4        0.729     0.411\n",
      "4   communities  elrp_MLP      RBO   5        0.743     0.411\n",
      "5   communities  elrp_MLP      RBO   6        0.744     0.411\n",
      "6   communities  elrp_MLP      RBO   7        0.740     0.411\n",
      "7   communities  elrp_MLP      RBO   8        0.757     0.411\n",
      "8   communities  elrp_MLP      RBO   9        0.759     0.411\n",
      "9   communities  elrp_MLP      RBO  10        0.768     0.411\n",
      "10  communities  elrp_MLP      RBO  11        0.777     0.411\n",
      "11  communities  elrp_MLP      RBO  12        0.782     0.411\n",
      "12  communities  elrp_MLP      RBO  13        0.790     0.411\n",
      "13  communities  elrp_MLP      RBO  14        0.794     0.411\n",
      "14  communities  elrp_MLP      RBO  15        0.795     0.411\n",
      "15  communities  elrp_MLP      RBO  16        0.792     0.411\n",
      "16  communities  elrp_MLP      RBO  17        0.794     0.411\n",
      "17  communities  elrp_MLP      RBO  18        0.799     0.411\n",
      "18  communities  elrp_MLP      RBO  19        0.807     0.411\n",
      "19  communities  elrp_MLP      RBO  20        0.812     0.411\n",
      "20  communities  elrp_MLP      RBO  21        0.815     0.411\n",
      "21  communities  elrp_MLP      RBO  22        0.818     0.411\n",
      "22  communities  elrp_MLP      RBO  23        0.819     0.411\n",
      "23  communities  elrp_MLP      RBO  24        0.821     0.411\n",
      "24  communities  elrp_MLP      RBO  25        0.822     0.411\n",
      "25  communities  elrp_MLP      RBO  26        0.822     0.411\n",
      "26  communities  elrp_MLP      RBO  27        0.822     0.411\n",
      "27  communities  elrp_MLP      RBO  28        0.823     0.411\n",
      "28  communities  elrp_MLP      RBO  29        0.825     0.411\n",
      "29  communities  elrp_MLP      RBO  30        0.827     0.411\n",
      "30  communities  elrp_MLP  Jaccard   1        1.000     0.411\n",
      "31  communities  elrp_MLP  Jaccard   2        0.333     0.411\n",
      "32  communities  elrp_MLP  Jaccard   3        0.500     0.411\n",
      "33  communities  elrp_MLP  Jaccard   4        0.600     0.411\n",
      "34  communities  elrp_MLP  Jaccard   5        0.667     0.411\n",
      "35  communities  elrp_MLP  Jaccard   6        0.607     0.411\n",
      "36  communities  elrp_MLP  Jaccard   7        0.556     0.411\n",
      "37  communities  elrp_MLP  Jaccard   8        0.778     0.411\n",
      "38  communities  elrp_MLP  Jaccard   9        0.636     0.411\n",
      "39  communities  elrp_MLP  Jaccard  10        0.742     0.411\n",
      "40  communities  elrp_MLP  Jaccard  11        0.763     0.411\n",
      "41  communities  elrp_MLP  Jaccard  12        0.723     0.411\n",
      "42  communities  elrp_MLP  Jaccard  13        0.795     0.411\n",
      "43  communities  elrp_MLP  Jaccard  14        0.750     0.411\n",
      "44  communities  elrp_MLP  Jaccard  15        0.667     0.411\n",
      "45  communities  elrp_MLP  Jaccard  16        0.600     0.411\n",
      "46  communities  elrp_MLP  Jaccard  17        0.700     0.411\n",
      "47  communities  elrp_MLP  Jaccard  18        0.800     0.411\n",
      "48  communities  elrp_MLP  Jaccard  19        0.900     0.411\n",
      "49  communities  elrp_MLP  Jaccard  20        0.818     0.411\n",
      "50  communities  elrp_MLP  Jaccard  21        0.788     0.411\n",
      "51  communities  elrp_MLP  Jaccard  22        0.797     0.411\n",
      "52  communities  elrp_MLP  Jaccard  23        0.736     0.411\n",
      "53  communities  elrp_MLP  Jaccard  24        0.746     0.411\n",
      "54  communities  elrp_MLP  Jaccard  25        0.724     0.411\n",
      "55  communities  elrp_MLP  Jaccard  26        0.705     0.411\n",
      "56  communities  elrp_MLP  Jaccard  27        0.715     0.411\n",
      "57  communities  elrp_MLP  Jaccard  28        0.723     0.411\n",
      "58  communities  elrp_MLP  Jaccard  29        0.785     0.411\n",
      "59  communities  elrp_MLP  Jaccard  30        0.818     0.411\n",
      "          data model   Entropy    Purity\n",
      "0  communities   MLP  0.162327  0.852243\n"
     ]
    }
   ],
   "source": [
    "from deepexplain.tensorflow import DeepExplain\n",
    "importance_func ='elrp'\n",
    "\n",
    "model_class_mlp_dl=imlreliability.feature_importance.feature_impoClass_MLP(data_class,\n",
    "                                                                           \n",
    "                 importance_func=importance_func,\n",
    "                 n_repeat=3,split_proportion=0.7,\n",
    "                rand_index=1)\n",
    "model_class_mlp_dl.fit()\n",
    "model_class_mlp_dl.get_consistency(data_name='communities', estimator_name='MLP',impotance_func_name='elrp')\n",
    "print(model_class_mlp_dl.accuracy)\n",
    "print(model_class_mlp_dl.consistency)\n",
    "print(model_class_mlp_dl.prediction_consistency)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
